{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagoigfraga/pesquisa_sisrec_interacoes/blob/main/prev_proxartigo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_k5Lf3IxlS_",
        "outputId": "13784bdb-4ff8-4086-c580-483601b84fcf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5wwn6239R0-",
        "outputId": "1a063b76-ffe0-4cb0-a236-61486963b6d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting river\n",
            "  Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from river) (2.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.1->river) (1.16.0)\n",
            "Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m41.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: river\n",
            "Successfully installed river-0.21.2\n"
          ]
        }
      ],
      "source": [
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8ElIu7HxeLL",
        "outputId": "26367c55-edec-4b5a-a058-f6aed3f181ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando dataset de /content/drive/MyDrive/Pesquisa2024/dataset_otimizado.csv...\n",
            "\n",
            "=== Análise das Amostras ===\n",
            "Total de amostras: 2,987,469\n",
            "Período: 2017-10-01 03:00:00.026000 até 2017-10-17 23:51:27.187000\n",
            "Usuários únicos: 322,892\n",
            "Artigos únicos: 45,695\n",
            "\n",
            "Verificação de integridade:\n",
            "\n",
            "=== Análise Temporal ===\n",
            "\n",
            "Distribuição por hora do dia:\n",
            "hour\n",
            "0     126548\n",
            "1     120732\n",
            "2      94289\n",
            "3      61809\n",
            "4      32813\n",
            "5      18558\n",
            "6      14518\n",
            "7      16619\n",
            "8      32107\n",
            "9      72778\n",
            "10    129329\n",
            "11    184405\n",
            "12    161904\n",
            "13    168728\n",
            "14    182451\n",
            "15    187951\n",
            "16    201597\n",
            "17    191366\n",
            "18    190417\n",
            "19    192196\n",
            "20    179987\n",
            "21    152274\n",
            "22    141080\n",
            "23    133013\n",
            "dtype: int64\n",
            "\n",
            "Distribuição por dia:\n",
            "day\n",
            "2017-10-01     94056\n",
            "2017-10-02    303177\n",
            "2017-10-03    261159\n",
            "2017-10-04    215415\n",
            "2017-10-05    190003\n",
            "2017-10-06    207646\n",
            "2017-10-07    139323\n",
            "2017-10-08    108110\n",
            "2017-10-09    248208\n",
            "2017-10-10    282391\n",
            "2017-10-11    238969\n",
            "2017-10-12    121467\n",
            "2017-10-13    180723\n",
            "2017-10-14     95216\n",
            "2017-10-15     92163\n",
            "2017-10-16    189779\n",
            "2017-10-17     19664\n",
            "dtype: int64\n",
            "\n",
            "Intervalos entre interações (segundos):\n",
            "count    2.987468e+06\n",
            "mean     4.878670e-01\n",
            "std      1.723337e+01\n",
            "min      0.000000e+00\n",
            "25%      8.900000e-02\n",
            "50%      2.260000e-01\n",
            "75%      5.060000e-01\n",
            "max      2.942229e+04\n",
            "Name: time_diff, dtype: float64\n",
            "\n",
            "Iniciando validação temporal...\n",
            "\n",
            "Dados originais de validação: 890,341 registros\n",
            "Limitando validação a 10,000 amostras aleatórias\n",
            "\n",
            "Dados de treino: 2,077,464 registros\n",
            "Dados de validação: 10,000 registros\n",
            "\n",
            "Treinando modelo...\n",
            "Processados 0 registros de treino\n",
            "Processados 100,000 registros de treino\n",
            "Processados 200,000 registros de treino\n",
            "Processados 300,000 registros de treino\n",
            "Processados 400,000 registros de treino\n",
            "Processados 500,000 registros de treino\n",
            "Processados 600,000 registros de treino\n",
            "Processados 700,000 registros de treino\n",
            "Processados 800,000 registros de treino\n",
            "Processados 900,000 registros de treino\n",
            "Processados 1,000,000 registros de treino\n",
            "Processados 1,100,000 registros de treino\n",
            "Processados 1,200,000 registros de treino\n",
            "Processados 1,300,000 registros de treino\n",
            "Processados 1,400,000 registros de treino\n",
            "Processados 1,500,000 registros de treino\n",
            "Processados 1,600,000 registros de treino\n",
            "Processados 1,700,000 registros de treino\n",
            "Processados 1,800,000 registros de treino\n",
            "Processados 1,900,000 registros de treino\n",
            "Processados 2,000,000 registros de treino\n",
            "\n",
            "Validando modelo...\n",
            "Processados 1,000 registros de validação\n",
            "Processados 2,000 registros de validação\n",
            "Processados 3,000 registros de validação\n",
            "Processados 4,000 registros de validação\n",
            "Processados 5,000 registros de validação\n",
            "Processados 6,000 registros de validação\n",
            "Processados 7,000 registros de validação\n",
            "Processados 8,000 registros de validação\n",
            "Processados 9,000 registros de validação\n",
            "Processados 10,000 registros de validação\n",
            "\n",
            "Métricas de Validação Random Forest:\n",
            "accuracy: 0.0034\n",
            "macro_f1: 0.0000\n",
            "micro_f1: 0.0034\n",
            "macro_precision: 0.0000\n",
            "micro_precision: 0.0034\n",
            "macro_recall: 0.0002\n",
            "micro_recall: 0.0034\n",
            "\n",
            "Métricas de Validação Baseline:\n",
            "accuracy: 0.0000\n",
            "macro_f1: 0.0000\n",
            "micro_f1: 0.0000\n",
            "\n",
            "Analisando amostras...\n",
            "\n",
            "=== Análise das Amostras ===\n",
            "Total de amostras: 2,987,469\n",
            "Período: 2017-10-01 03:00:00.026000 até 2017-10-17 23:51:27.187000\n",
            "Usuários únicos: 322,892\n",
            "Artigos únicos: 45,695\n",
            "\n",
            "Verificação de integridade:\n",
            "\n",
            "=== Análise Temporal ===\n",
            "\n",
            "Distribuição por hora do dia:\n",
            "hour\n",
            "0     126548\n",
            "1     120732\n",
            "2      94289\n",
            "3      61809\n",
            "4      32813\n",
            "5      18558\n",
            "6      14518\n",
            "7      16619\n",
            "8      32107\n",
            "9      72778\n",
            "10    129329\n",
            "11    184405\n",
            "12    161904\n",
            "13    168728\n",
            "14    182451\n",
            "15    187951\n",
            "16    201597\n",
            "17    191366\n",
            "18    190417\n",
            "19    192196\n",
            "20    179987\n",
            "21    152274\n",
            "22    141080\n",
            "23    133013\n",
            "dtype: int64\n",
            "\n",
            "Distribuição por dia:\n",
            "day\n",
            "2017-10-01     94056\n",
            "2017-10-02    303177\n",
            "2017-10-03    261159\n",
            "2017-10-04    215415\n",
            "2017-10-05    190003\n",
            "2017-10-06    207646\n",
            "2017-10-07    139323\n",
            "2017-10-08    108110\n",
            "2017-10-09    248208\n",
            "2017-10-10    282391\n",
            "2017-10-11    238969\n",
            "2017-10-12    121467\n",
            "2017-10-13    180723\n",
            "2017-10-14     95216\n",
            "2017-10-15     92163\n",
            "2017-10-16    189779\n",
            "2017-10-17     19664\n",
            "dtype: int64\n",
            "\n",
            "Intervalos entre interações (segundos):\n",
            "count    2.987468e+06\n",
            "mean     4.878670e-01\n",
            "std      1.723337e+01\n",
            "min      0.000000e+00\n",
            "25%      8.900000e-02\n",
            "50%      2.260000e-01\n",
            "75%      5.060000e-01\n",
            "max      2.942229e+04\n",
            "Name: time_diff, dtype: float64\n",
            "\n",
            "Selecionados 7 dias ativos com 937,981 registros\n",
            "\n",
            "Processando janela 1/334\n",
            "Registros na janela: 7,765\n",
            "\n",
            "Processando janela 2/334\n",
            "Registros na janela: 6,715\n",
            "\n",
            "Processando janela 3/334\n",
            "Registros na janela: 8,042\n",
            "\n",
            "Processando janela 4/334\n",
            "Registros na janela: 8,848\n",
            "\n",
            "Processando janela 5/334\n",
            "Registros na janela: 7,121\n",
            "\n",
            "Processando janela 6/334\n",
            "Registros na janela: 5,302\n",
            "\n",
            "Processando janela 7/334\n",
            "Registros na janela: 3,765\n",
            "\n",
            "Processando janela 8/334\n",
            "Registros na janela: 2,602\n",
            "\n",
            "Processando janela 9/334\n",
            "Registros na janela: 1,854\n",
            "\n",
            "Processando janela 10/334\n",
            "Registros na janela: 1,337\n",
            "\n",
            "Progresso: 10/334 janelas processadas\n",
            "Registros processados: 53,351\n",
            "\n",
            "Métricas atuais:\n",
            "\n",
            "Métricas Random Forest:\n",
            "accuracy: 0.0902339398440401\n",
            "macro_f1: 0.01930600143849117\n",
            "micro_f1: 0.09023393984404009\n",
            "macro_precision: 0.0187828222286783\n",
            "micro_precision: 0.0902339398440401\n",
            "macro_recall: 0.023847726165088776\n",
            "micro_recall: 0.0902339398440401\n",
            "\n",
            "Métricas Baseline:\n",
            "accuracy: 0.01855976243504083\n",
            "macro_f1: 0.0003706139831366364\n",
            "micro_f1: 0.01855976243504083\n",
            "\n",
            "Processando janela 11/334\n",
            "Registros na janela: 1,143\n",
            "\n",
            "Processando janela 12/334\n",
            "Registros na janela: 1,037\n",
            "\n",
            "Processando janela 13/334\n",
            "Registros na janela: 963\n",
            "\n",
            "Processando janela 14/334\n",
            "Registros na janela: 1,061\n",
            "\n",
            "Processando janela 15/334\n",
            "Registros na janela: 1,169\n",
            "\n",
            "Processando janela 16/334\n",
            "Registros na janela: 1,449\n",
            "\n",
            "Processando janela 17/334\n",
            "Registros na janela: 2,565\n",
            "\n",
            "Processando janela 18/334\n",
            "Registros na janela: 4,396\n",
            "\n",
            "Processando janela 19/334\n",
            "Registros na janela: 5,991\n",
            "\n",
            "Processando janela 20/334\n",
            "Registros na janela: 8,150\n",
            "\n",
            "Progresso: 20/334 janelas processadas\n",
            "Registros processados: 81,275\n",
            "\n",
            "Métricas atuais:\n",
            "\n",
            "Métricas Random Forest:\n",
            "accuracy: 0.08967291436971701\n",
            "macro_f1: 0.018570801718944294\n",
            "micro_f1: 0.08967291436971701\n",
            "macro_precision: 0.017709382512519026\n",
            "micro_precision: 0.08967291436971701\n",
            "macro_recall: 0.02346752614603436\n",
            "micro_recall: 0.08967291436971701\n",
            "\n",
            "Métricas Baseline:\n",
            "accuracy: 0.018368846436443792\n",
            "macro_f1: 0.0003667382770905787\n",
            "micro_f1: 0.018368846436443792\n",
            "\n",
            "Processando janela 21/334\n",
            "Registros na janela: 11,266\n",
            "\n",
            "Processando janela 22/334\n",
            "Registros na janela: 15,431\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from river import forest, metrics, drift, stats\n",
        "import random\n",
        "import hashlib\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from collections import Counter, defaultdict\n",
        "import os\n",
        "import traceback\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Definir sementes para reprodutibilidade\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def convert_unix_timestamp(ts):\n",
        "    \"\"\"Converte timestamp Unix em milissegundos para datetime\"\"\"\n",
        "    return pd.to_datetime(ts, unit=\"ms\")\n",
        "\n",
        "\n",
        "def consistent_hash(value):\n",
        "    \"\"\"Função de hash consistente usando SHA-256\"\"\"\n",
        "    return int(hashlib.sha256(str(value).encode(\"utf-8\")).hexdigest(), 16) % 1000\n",
        "\n",
        "\n",
        "def analyze_temporal_distribution(dataset):\n",
        "    \"\"\"\n",
        "    Analisa a distribuição temporal dos dados\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Análise Temporal ===\")\n",
        "\n",
        "    # Converte timestamps\n",
        "    dataset[\"hour\"] = dataset[\"timestamp\"].dt.hour\n",
        "    dataset[\"day\"] = dataset[\"timestamp\"].dt.date\n",
        "\n",
        "    # Análise por hora\n",
        "    hourly_dist = dataset.groupby(\"hour\").size()\n",
        "    print(\"\\nDistribuição por hora do dia:\")\n",
        "    print(hourly_dist)\n",
        "\n",
        "    # Análise por dia\n",
        "    daily_dist = dataset.groupby(\"day\").size()\n",
        "    print(\"\\nDistribuição por dia:\")\n",
        "    print(daily_dist)\n",
        "\n",
        "    # Calcula intervalos entre interações\n",
        "    dataset = dataset.sort_values(\"timestamp\")\n",
        "    dataset[\"time_diff\"] = dataset[\"timestamp\"].diff().dt.total_seconds()\n",
        "\n",
        "    print(\"\\nIntervalos entre interações (segundos):\")\n",
        "    print(dataset[\"time_diff\"].describe())\n",
        "\n",
        "    return hourly_dist, daily_dist\n",
        "\n",
        "\n",
        "class OnlineNewsRecommender:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_models=15,\n",
        "        drift_detector=drift.ADWIN(delta=0.002),\n",
        "        top_k=10,\n",
        "    ):\n",
        "        # Mantém histórico temporal\n",
        "        self.temporal_weights = {}  # Pesos temporais para artigos\n",
        "        self.article_timestamps = defaultdict(list)  # Últimos timestamps por artigo\n",
        "        self.time_window = 3600 * 6  # 6 horas em segundos\n",
        "        self.last_user_interaction = {}  # Para controle de sessões\n",
        "\n",
        "        # Inicializa um normalizador para cada feature numérica\n",
        "        self.scalers = {\n",
        "            \"user_id_hash\": stats.Mean(),\n",
        "            \"article_id_hash\": stats.Mean(),\n",
        "            \"hour_sin\": stats.Mean(),\n",
        "            \"hour_cos\": stats.Mean(),\n",
        "            \"day_sin\": stats.Mean(),\n",
        "            \"day_cos\": stats.Mean(),\n",
        "            \"month_sin\": stats.Mean(),\n",
        "            \"month_cos\": stats.Mean(),\n",
        "            \"hour\": stats.Mean(),\n",
        "            \"article_popularity\": stats.Mean(),\n",
        "            \"user_activity\": stats.Mean(),\n",
        "            \"temporal_weight\": stats.Mean(),\n",
        "            \"time_since_last_interaction\": stats.Mean(),\n",
        "        }\n",
        "\n",
        "        self.vars = {k: stats.Var() for k in self.scalers.keys()}\n",
        "\n",
        "        # Modelo adaptado para classificação multiclasse\n",
        "        self.model = forest.ARFClassifier(\n",
        "            n_models=n_models,\n",
        "            drift_detector=drift_detector,\n",
        "            grace_period=50,\n",
        "            max_features=\"sqrt\",\n",
        "            seed=42,\n",
        "            leaf_prediction=\"nb\",\n",
        "        )\n",
        "\n",
        "        # Métricas adaptadas para classificação multiclasse\n",
        "        self.metrics = {\n",
        "            \"random_forest\": {\n",
        "                \"accuracy\": metrics.Accuracy(),\n",
        "                \"macro_f1\": metrics.MacroF1(),\n",
        "                \"micro_f1\": metrics.MicroF1(),\n",
        "                \"macro_precision\": metrics.MacroPrecision(),\n",
        "                \"micro_precision\": metrics.MicroPrecision(),\n",
        "                \"macro_recall\": metrics.MacroRecall(),\n",
        "                \"micro_recall\": metrics.MicroRecall(),\n",
        "            },\n",
        "            \"popular_baseline\": {\n",
        "                \"accuracy\": metrics.Accuracy(),\n",
        "                \"macro_f1\": metrics.MacroF1(),\n",
        "                \"micro_f1\": metrics.MicroF1(),\n",
        "            },\n",
        "        }\n",
        "\n",
        "        # Contadores e estado\n",
        "        self.article_counter = Counter()\n",
        "        self.user_counter = Counter()\n",
        "        self.current_top = set()\n",
        "        self.top_k = top_k\n",
        "        self.comparison_results = []\n",
        "\n",
        "    def _calculate_temporal_weight(self, article_id, current_timestamp):\n",
        "        \"\"\"\n",
        "        Calcula peso temporal do artigo baseado em suas interações recentes\n",
        "        \"\"\"\n",
        "        recent_timestamps = [\n",
        "            ts\n",
        "            for ts in self.article_timestamps[article_id]\n",
        "            if (current_timestamp - ts).total_seconds() <= self.time_window\n",
        "        ]\n",
        "\n",
        "        if not recent_timestamps:\n",
        "            return 0.0\n",
        "\n",
        "        # Peso decai exponencialmente com o tempo\n",
        "        weights = np.exp(\n",
        "            -0.1\n",
        "            * np.array(\n",
        "                [\n",
        "                    (current_timestamp - ts).total_seconds()\n",
        "                    / 3600  # Converte para horas\n",
        "                    for ts in recent_timestamps\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return np.mean(weights)\n",
        "\n",
        "    def _normalize_feature(self, name, value):\n",
        "        \"\"\"Normaliza uma feature usando média e variância online\"\"\"\n",
        "        # Atualiza estatísticas\n",
        "        self.scalers[name].update(value)\n",
        "        self.vars[name].update(value)\n",
        "\n",
        "        # Calcula z-score\n",
        "        mean = self.scalers[name].get()\n",
        "        var = self.vars[name].get()\n",
        "        std = np.sqrt(var) if var > 0 else 1\n",
        "\n",
        "        return (value - mean) / (std + 1e-8)\n",
        "\n",
        "    def _extract_features(self, x):\n",
        "        \"\"\"Extrai as features do exemplo de entrada, incluindo informação de sessão\"\"\"\n",
        "        timestamp = convert_unix_timestamp(x[\"click_timestamp\"])\n",
        "\n",
        "        # Lógica de sessão\n",
        "        if hasattr(self, \"last_user_interaction\"):\n",
        "            time_diff = (\n",
        "                timestamp - self.last_user_interaction.get(x[\"user_id\"], timestamp)\n",
        "            ).total_seconds()\n",
        "            is_new_session = time_diff > 1800  # Nova sessão após 30 minutos\n",
        "        else:\n",
        "            time_diff = 0\n",
        "            is_new_session = True\n",
        "\n",
        "        # Atualiza último timestamp de interação do usuário\n",
        "        self.last_user_interaction[x[\"user_id\"]] = timestamp\n",
        "\n",
        "        features = {\n",
        "            \"user_id_hash\": consistent_hash(x[\"user_id\"]),\n",
        "            \"article_id_hash\": consistent_hash(x[\"click_article_id\"]),\n",
        "            \"hour_sin\": np.sin(2 * np.pi * timestamp.hour / 24),\n",
        "            \"hour_cos\": np.cos(2 * np.pi * timestamp.hour / 24),\n",
        "            \"day_sin\": np.sin(2 * np.pi * timestamp.dayofweek / 7),\n",
        "            \"day_cos\": np.cos(2 * np.pi * timestamp.dayofweek / 7),\n",
        "            \"month_sin\": np.sin(2 * np.pi * timestamp.month / 12),\n",
        "            \"month_cos\": np.cos(2 * np.pi * timestamp.month / 12),\n",
        "            \"hour\": timestamp.hour,\n",
        "            \"time_since_last_interaction\": time_diff,\n",
        "            \"is_new_session\": int(is_new_session),\n",
        "        }\n",
        "\n",
        "        # Calcula popularidade excluindo a interação atual\n",
        "        article_count = self.article_counter.get(x[\"click_article_id\"], 0)\n",
        "        user_count = self.user_counter.get(x[\"user_id\"], 0)\n",
        "        total_interactions = sum(self.article_counter.values()) or 1\n",
        "        total_users = sum(self.user_counter.values()) or 1\n",
        "\n",
        "        temporal_weight = self._calculate_temporal_weight(\n",
        "            x[\"click_article_id\"], timestamp\n",
        "        )\n",
        "\n",
        "        features.update(\n",
        "            {\n",
        "                \"article_popularity\": article_count / total_interactions,\n",
        "                \"user_activity\": user_count / total_users,\n",
        "                \"temporal_weight\": temporal_weight,\n",
        "                \"is_business_hour\": int(9 <= timestamp.hour <= 18),\n",
        "                \"is_peak_hour\": int(10 <= timestamp.hour <= 16),\n",
        "                \"day_of_week\": timestamp.dayofweek,\n",
        "                \"is_weekend\": int(timestamp.dayofweek >= 5),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Normaliza features numéricas\n",
        "        normalized_features = {\n",
        "            name: self._normalize_feature(name, value)\n",
        "            for name, value in features.items()\n",
        "            if name in self.scalers\n",
        "        }\n",
        "\n",
        "        # Adiciona features categóricas\n",
        "        normalized_features.update(\n",
        "            {\n",
        "                k: features[k]\n",
        "                for k in [\n",
        "                    \"is_business_hour\",\n",
        "                    \"is_peak_hour\",\n",
        "                    \"day_of_week\",\n",
        "                    \"is_weekend\",\n",
        "                    \"is_new_session\",\n",
        "                ]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        return normalized_features\n",
        "\n",
        "    def learn_one(self, x):\n",
        "        \"\"\"Aprende com um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "\n",
        "        # Verifica se artigo está no top_k\n",
        "        is_top_k = x[\"click_article_id\"] in self.current_top\n",
        "\n",
        "        if hasattr(self, \"previous_features\"):\n",
        "            # Só prevê próximo artigo se for mesmo usuário e mesma sessão\n",
        "            if x[\"user_id\"] == self.previous_user_id and not features[\"is_new_session\"]:\n",
        "\n",
        "                # O target agora é o artigo atual\n",
        "                target = x[\"click_article_id\"]\n",
        "\n",
        "                # Predição usando Random Forest\n",
        "                rf_pred = self.model.predict_one(self.previous_features)\n",
        "\n",
        "                # Predição usando baseline de popularidade\n",
        "                top_articles = [\n",
        "                    article\n",
        "                    for article, _ in self.article_counter.most_common(self.top_k)\n",
        "                ]\n",
        "                popular_pred = top_articles[0] if top_articles else None\n",
        "\n",
        "                # Atualiza métricas para ambos os métodos\n",
        "                if rf_pred is not None:\n",
        "                    for metric in self.metrics[\"random_forest\"].values():\n",
        "                        metric.update(target, rf_pred)\n",
        "\n",
        "                if popular_pred is not None:\n",
        "                    for metric in self.metrics[\"popular_baseline\"].values():\n",
        "                        metric.update(target, popular_pred)\n",
        "\n",
        "                # Registra resultados comparativos\n",
        "                self.comparison_results.append(\n",
        "                    {\n",
        "                        \"timestamp\": x[\"timestamp\"],\n",
        "                        \"rf_prediction\": rf_pred,\n",
        "                        \"popular_prediction\": popular_pred,\n",
        "                        \"actual\": target,\n",
        "                        \"rf_correct\": int(rf_pred == target),\n",
        "                        \"popular_correct\": int(popular_pred == target),\n",
        "                    }\n",
        "                )\n",
        "\n",
        "                # Atualiza modelo com nova observação\n",
        "                self.model.learn_one(self.previous_features, target)\n",
        "\n",
        "        # Guarda informações para próxima iteração\n",
        "        self.previous_features = features\n",
        "        self.previous_user_id = x[\"user_id\"]\n",
        "\n",
        "        # Atualiza contadores\n",
        "        self.article_counter[x[\"click_article_id\"]] += 1\n",
        "        self.user_counter[x[\"user_id\"]] += 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        \"\"\"Faz predição para um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "\n",
        "        if not hasattr(self, \"previous_features\"):\n",
        "            return None\n",
        "\n",
        "        return self.model.predict_one(features)\n",
        "\n",
        "    def predict_proba_one(self, x):\n",
        "        \"\"\"Retorna probabilidades para um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "\n",
        "        if not hasattr(self, \"previous_features\"):\n",
        "            return {0: 0.5, 1: 0.5}\n",
        "\n",
        "        return self.model.predict_proba_one(features)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Salva o modelo completo\"\"\"\n",
        "        with open(path, \"wb\") as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\"Carrega o modelo completo\"\"\"\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "\n",
        "class NewsRecommenderSystem:\n",
        "    \"\"\"Sistema para gerenciar treinamento e predição do recomendador de notícias.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path=None, max_samples=None, timestamp=None, config=None):\n",
        "        \"\"\"\n",
        "        Inicializa o sistema de recomendação.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model_path : str, optional\n",
        "            Caminho para o modelo existente.\n",
        "        max_samples : int, optional\n",
        "            Número máximo de amostras a serem processadas.\n",
        "        timestamp : str, optional\n",
        "            Timestamp para nomear arquivos.\n",
        "        config : dict, optional\n",
        "            Configurações do modelo.\n",
        "        \"\"\"\n",
        "        self.model = None\n",
        "        self.history = {\n",
        "            \"processed_records\": 0,\n",
        "            \"start_time\": datetime.now(),\n",
        "            \"metrics\": {},\n",
        "            \"dataset_info\": {},\n",
        "            \"model_info\": {},\n",
        "            \"config\": config or {},\n",
        "        }\n",
        "        self.max_samples = max_samples\n",
        "        self.timestamp = timestamp or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        if model_path:\n",
        "            self._load_existing_model(model_path)\n",
        "        else:\n",
        "            self._initialize_new_model()\n",
        "\n",
        "    def _load_existing_model(self, model_path):\n",
        "        \"\"\"Carrega modelo existente.\"\"\"\n",
        "        try:\n",
        "            self.model = OnlineNewsRecommender.load(model_path)\n",
        "            self.history[\"model_info\"] = {\n",
        "                \"original_model_path\": model_path,\n",
        "                \"load_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            }\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Erro ao carregar modelo: {str(e)}\")\n",
        "\n",
        "    def _initialize_new_model(self):\n",
        "        \"\"\"Inicializa novo modelo.\"\"\"\n",
        "        n_models = self.history[\"config\"].get(\"n_models\", 15)\n",
        "        delta = self.history[\"config\"].get(\"delta\", 0.002)  # Ajustado\n",
        "        top_k = self.history[\"config\"].get(\"top_k\", 10)\n",
        "\n",
        "        drift_detector = drift.ADWIN(delta=delta)\n",
        "\n",
        "        self.model = OnlineNewsRecommender(\n",
        "            n_models=n_models, drift_detector=drift_detector, top_k=top_k\n",
        "        )\n",
        "\n",
        "        # Salva as configurações do classificador\n",
        "        self.history[\"model_info\"] = {\n",
        "            \"initialization_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"model_type\": \"OnlineNewsRecommender\",\n",
        "            \"n_models\": n_models,\n",
        "            \"drift_detector\": type(drift_detector).__name__,\n",
        "            \"drift_detector_params\": drift_detector.__dict__,\n",
        "            \"top_k\": top_k,\n",
        "            \"model_params\": {\n",
        "                \"n_models\": n_models,\n",
        "                \"grace_period\": self.model.model.grace_period,\n",
        "                \"max_features\": self.model.model.max_features,\n",
        "                \"seed\": self.model.model.seed,\n",
        "                \"leaf_prediction\": self.model.model.leaf_prediction,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def validate_dataset(self, dataset: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        Valida se o dataset tem as colunas necessárias.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser validado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bool\n",
        "            True se o dataset é válido, levanta ValueError caso contrário.\n",
        "        \"\"\"\n",
        "        required_columns = [\"user_id\", \"click_article_id\", \"click_timestamp\"]\n",
        "        missing_columns = [\n",
        "            col for col in required_columns if col not in dataset.columns\n",
        "        ]\n",
        "\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Colunas faltando no dataset: {missing_columns}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _update_final_metrics(self):\n",
        "        try:\n",
        "            n_drifts = sum(\n",
        "                model.drift_detector.n_detections\n",
        "                for model in self.model.model._models\n",
        "                if hasattr(model, \"drift_detector\")\n",
        "            )\n",
        "            n_warnings = sum(\n",
        "                model.drift_detector.n_warnings\n",
        "                for model in self.model.model._models\n",
        "                if hasattr(model, \"drift_detector\")\n",
        "            )\n",
        "\n",
        "            metrics_data = {\n",
        "                \"random_forest\": {},\n",
        "                \"popular_baseline\": {},\n",
        "                \"additional_info\": {\n",
        "                    \"processed_records\": self.history[\"processed_records\"],\n",
        "                    \"processing_time\": str(datetime.now() - self.history[\"start_time\"]),\n",
        "                    \"n_drifts\": n_drifts,\n",
        "                    \"n_warnings\": n_warnings\n",
        "                }\n",
        "            }\n",
        "\n",
        "            # Atualiza métricas Random Forest\n",
        "            for name, metric in self.model.metrics[\"random_forest\"].items():\n",
        "                metrics_data[\"random_forest\"][name] = metric.get()\n",
        "\n",
        "            # Atualiza métricas Baseline\n",
        "            for name, metric in self.model.metrics[\"popular_baseline\"].items():\n",
        "                metrics_data[\"popular_baseline\"][name] = metric.get()\n",
        "\n",
        "            self.history[\"metrics\"] = metrics_data\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro atualizando métricas: {str(e)}\")\n",
        "            self.history[\"metrics\"] = {\n",
        "                \"error\": str(e),\n",
        "                \"processed_records\": self.history[\"processed_records\"],\n",
        "                \"processing_time\": str(datetime.now() - self.history[\"start_time\"])\n",
        "            }\n",
        "\n",
        "    def analyze_samples(self, dataset):\n",
        "        \"\"\"\n",
        "        Analisa e mostra informações sobre as amostras do dataset com validações adicionais.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== Análise das Amostras ===\")\n",
        "\n",
        "            # Verifica duplicações\n",
        "            duplicates = dataset.duplicated().sum()\n",
        "            if duplicates > 0:\n",
        "                print(f\"Aviso: Encontradas {duplicates:,} linhas duplicadas\")\n",
        "                print(\"Removendo duplicatas...\")\n",
        "                dataset = dataset.drop_duplicates()\n",
        "\n",
        "            # Contagem de registros\n",
        "            total_samples = len(dataset)\n",
        "            total_interactions = (\n",
        "                dataset.groupby([\"user_id\", \"click_article_id\", \"click_timestamp\"])\n",
        "                .size()\n",
        "                .sum()\n",
        "            )\n",
        "\n",
        "            if total_samples != total_interactions:\n",
        "                print(f\"Aviso: Possível inconsistência na contagem de amostras\")\n",
        "                print(f\"Total de linhas: {total_samples:,}\")\n",
        "                print(f\"Total de interações únicas: {total_interactions:,}\")\n",
        "\n",
        "            print(f\"Total de amostras: {total_interactions:,}\")\n",
        "            print(\n",
        "                f\"Período: {dataset['timestamp'].min()} até {dataset['timestamp'].max()}\"\n",
        "            )\n",
        "            print(f\"Usuários únicos: {dataset['user_id'].nunique():,}\")\n",
        "            print(f\"Artigos únicos: {dataset['click_article_id'].nunique():,}\")\n",
        "\n",
        "            # Verificar integridade dos dados\n",
        "            print(\"\\nVerificação de integridade:\")\n",
        "            null_counts = dataset.isnull().sum()\n",
        "            if null_counts.any():\n",
        "                print(\"Valores nulos encontrados:\")\n",
        "                print(null_counts[null_counts > 0])\n",
        "                dataset = dataset.dropna()\n",
        "\n",
        "            # Análise de timestamps\n",
        "            invalid_timestamps = (\n",
        "                pd.to_datetime(dataset[\"click_timestamp\"], unit=\"ms\", errors=\"coerce\")\n",
        "                .isnull()\n",
        "                .sum()\n",
        "            )\n",
        "            if invalid_timestamps > 0:\n",
        "                print(f\"\\nAviso: {invalid_timestamps} timestamps inválidos encontrados\")\n",
        "                dataset = dataset.dropna(subset=[\"click_timestamp\"])\n",
        "\n",
        "            # Análise temporal\n",
        "            analyze_temporal_distribution(dataset)\n",
        "\n",
        "            return dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro analisando amostras: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return dataset\n",
        "\n",
        "    def validate_temporal(\n",
        "        self, dataset, validation_ratio=0.2, max_validation_samples=10000\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Realiza validação temporal do modelo usando uma proporção dos dados ativos\n",
        "        \"\"\"\n",
        "        print(\"\\nIniciando validação temporal...\")\n",
        "\n",
        "        daily_counts = dataset.groupby(dataset[\"timestamp\"].dt.date).size()\n",
        "        active_days = daily_counts[daily_counts > 1000].index\n",
        "        active_days = sorted(active_days)[:16]\n",
        "\n",
        "        dataset = dataset[dataset[\"timestamp\"].dt.date.isin(active_days)]\n",
        "        dataset = dataset.sort_values(\"timestamp\")\n",
        "\n",
        "        n_samples = len(dataset)\n",
        "        split_idx = int(n_samples * (1 - validation_ratio))\n",
        "        split_time = dataset.iloc[split_idx][\"timestamp\"]\n",
        "\n",
        "        train_data = dataset[dataset[\"timestamp\"] <= split_time]\n",
        "        val_data = dataset[dataset[\"timestamp\"] > split_time]\n",
        "\n",
        "        print(f\"\\nDados originais de validação: {len(val_data):,} registros\")\n",
        "        if len(val_data) > max_validation_samples:\n",
        "            print(\n",
        "                f\"Limitando validação a {max_validation_samples:,} amostras aleatórias\"\n",
        "            )\n",
        "            val_data = val_data.sample(n=max_validation_samples, random_state=42)\n",
        "\n",
        "        print(f\"\\nDados de treino: {len(train_data):,} registros\")\n",
        "        print(f\"Dados de validação: {len(val_data):,} registros\")\n",
        "\n",
        "        print(\"\\nTreinando modelo...\")\n",
        "        for idx, (_, row) in enumerate(train_data.iterrows()):\n",
        "            if idx % 100000 == 0:\n",
        "                print(f\"Processados {idx:,} registros de treino\")\n",
        "            self.model.learn_one(row.to_dict())\n",
        "\n",
        "        print(\"\\nValidando modelo...\")\n",
        "        processed = 0\n",
        "        val_metrics = {\n",
        "            \"random_forest\": {\n",
        "                \"accuracy\": metrics.Accuracy(),\n",
        "                \"macro_f1\": metrics.MacroF1(),\n",
        "                \"micro_f1\": metrics.MicroF1(),\n",
        "                \"macro_precision\": metrics.MacroPrecision(),\n",
        "                \"micro_precision\": metrics.MicroPrecision(),\n",
        "                \"macro_recall\": metrics.MacroRecall(),\n",
        "                \"micro_recall\": metrics.MicroRecall(),\n",
        "            },\n",
        "            \"popular_baseline\": {\n",
        "                \"accuracy\": metrics.Accuracy(),\n",
        "                \"macro_f1\": metrics.MacroF1(),\n",
        "                \"micro_f1\": metrics.MicroF1(),\n",
        "            },\n",
        "        }\n",
        "\n",
        "        predictions = []\n",
        "        for _, row in val_data.iterrows():\n",
        "            processed += 1\n",
        "            if processed % 1000 == 0:\n",
        "                print(f\"Processados {processed:,} registros de validação\")\n",
        "\n",
        "            x = row.to_dict()\n",
        "            rf_pred = self.model.predict_one(x)\n",
        "\n",
        "            top_articles = [\n",
        "                article\n",
        "                for article, _ in self.model.article_counter.most_common(\n",
        "                    self.model.top_k\n",
        "                )\n",
        "            ]\n",
        "            popular_pred = top_articles[0] if top_articles else None\n",
        "\n",
        "            target = x[\"click_article_id\"]\n",
        "\n",
        "            if rf_pred is not None:\n",
        "                for metric in val_metrics[\"random_forest\"].values():\n",
        "                    metric.update(target, rf_pred)\n",
        "\n",
        "            if popular_pred is not None:\n",
        "                for metric in val_metrics[\"popular_baseline\"].values():\n",
        "                    metric.update(target, popular_pred)\n",
        "\n",
        "            predictions.append(\n",
        "                {\n",
        "                    \"timestamp\": x[\"timestamp\"],\n",
        "                    \"user_id\": x[\"user_id\"],\n",
        "                    \"article_id\": x[\"click_article_id\"],\n",
        "                    \"rf_prediction\": rf_pred,\n",
        "                    \"popular_prediction\": popular_pred,\n",
        "                    \"target\": target,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\\nMétricas de Validação Random Forest:\")\n",
        "        for name, metric in val_metrics[\"random_forest\"].items():\n",
        "            print(f\"{name}: {metric.get():.4f}\")\n",
        "\n",
        "        print(\"\\nMétricas de Validação Baseline:\")\n",
        "        for name, metric in val_metrics[\"popular_baseline\"].items():\n",
        "            print(f\"{name}: {metric.get():.4f}\")\n",
        "\n",
        "        return val_metrics, predictions\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"Salva o histórico do processamento em um arquivo CSV.\"\"\"\n",
        "        try:\n",
        "            history_data = {\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"dataset_size\": self.history[\"dataset_info\"].get(\"total_records\"),\n",
        "                \"unique_users\": self.history[\"dataset_info\"].get(\"unique_users\"),\n",
        "                \"unique_articles\": self.history[\"dataset_info\"].get(\"unique_articles\"),\n",
        "                \"window_size\": self.history[\"dataset_info\"].get(\"window_size\"),\n",
        "                \"step_size\": self.history[\"dataset_info\"].get(\"step_size\"),\n",
        "                \"total_days\": self.history[\"dataset_info\"].get(\"total_days\"),\n",
        "                # Métricas Random Forest\n",
        "                \"rf_accuracy\": self.history[\"metrics\"][\"random_forest\"].get(\n",
        "                    \"final_accuracy\"\n",
        "                ),\n",
        "                \"rf_macro_f1\": self.history[\"metrics\"][\"random_forest\"].get(\"macro_f1\"),\n",
        "                \"rf_micro_f1\": self.history[\"metrics\"][\"random_forest\"].get(\"micro_f1\"),\n",
        "                \"rf_macro_precision\": self.history[\"metrics\"][\"random_forest\"].get(\n",
        "                    \"macro_precision\"\n",
        "                ),\n",
        "                \"rf_micro_precision\": self.history[\"metrics\"][\"random_forest\"].get(\n",
        "                    \"micro_precision\"\n",
        "                ),\n",
        "                \"rf_macro_recall\": self.history[\"metrics\"][\"random_forest\"].get(\n",
        "                    \"macro_recall\"\n",
        "                ),\n",
        "                \"rf_micro_recall\": self.history[\"metrics\"][\"random_forest\"].get(\n",
        "                    \"micro_recall\"\n",
        "                ),\n",
        "                # Métricas Baseline Popular\n",
        "                \"popular_accuracy\": self.history[\"metrics\"][\"popular_baseline\"].get(\n",
        "                    \"final_accuracy\"\n",
        "                ),\n",
        "                \"popular_macro_f1\": self.history[\"metrics\"][\"popular_baseline\"].get(\n",
        "                    \"macro_f1\"\n",
        "                ),\n",
        "                \"popular_micro_f1\": self.history[\"metrics\"][\"popular_baseline\"].get(\n",
        "                    \"micro_f1\"\n",
        "                ),\n",
        "                \"n_warnings\": self.history[\"metrics\"].get(\"n_warnings\"),\n",
        "                \"method\": \"next_article_prediction\",\n",
        "                \"comparison_results\": str(self.comparison_results),\n",
        "                \"model_path\": self.history[\"model_info\"].get(\"final_model_path\"),\n",
        "                \"original_model\": self.history[\"model_info\"].get(\"original_model_path\"),\n",
        "                \"model_type\": self.history[\"model_info\"].get(\"model_type\"),\n",
        "                \"n_models\": self.history[\"model_info\"].get(\"n_models\"),\n",
        "                \"delta\": self.history[\"config\"].get(\"delta\"),\n",
        "                \"drift_detector\": self.history[\"model_info\"].get(\"drift_detector\"),\n",
        "                \"drift_detector_params\": str(\n",
        "                    self.history[\"model_info\"].get(\"drift_detector_params\")\n",
        "                ),\n",
        "                \"top_k\": self.history[\"model_info\"].get(\"top_k\"),\n",
        "                \"model_params\": str(self.history[\"model_info\"].get(\"model_params\")),\n",
        "            }\n",
        "\n",
        "            history_df = pd.DataFrame([history_data])\n",
        "            base_path = \"/content/drive/MyDrive/Pesquisa2024/\"\n",
        "            filename = \"training_history.csv\"\n",
        "            filepath = os.path.join(base_path, filename)\n",
        "\n",
        "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                existing_df = pd.read_csv(filepath)\n",
        "                history_df = pd.concat([existing_df, history_df], ignore_index=True)\n",
        "\n",
        "            history_df.to_csv(filepath, index=False)\n",
        "            print(f\"\\nHistórico salvo em {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro salvando histórico: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    def train_and_predict_with_sliding_windows(\n",
        "        self, dataset, window_size=7200, step_size=3600, output_model_path=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Processa o dataset usando janelas deslizantes.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser processado.\n",
        "        window_size : int\n",
        "            Tamanho da janela em segundos (default: 1 hora)\n",
        "        step_size : int\n",
        "            Tamanho do passo em segundos (default: 30 minutos)\n",
        "        output_model_path : str, optional\n",
        "            Caminho para salvar o modelo atualizado.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.validate_dataset(dataset)\n",
        "            dataset[\"timestamp\"] = pd.to_datetime(dataset[\"click_timestamp\"], unit=\"ms\")\n",
        "            dataset = self.analyze_samples(dataset)\n",
        "\n",
        "            # Realiza validação temporal primeiro\n",
        "            val_metrics, val_predictions = self.validate_temporal(\n",
        "                dataset=dataset.copy(),\n",
        "                validation_ratio=self.history[\"config\"].get(\"validation_ratio\", 0.2),\n",
        "            )\n",
        "\n",
        "            # Análise das amostras\n",
        "            print(\"\\nAnalisando amostras...\")\n",
        "            dataset[\"timestamp\"] = pd.to_datetime(\n",
        "                dataset[\"click_timestamp\"], unit=\"ms\", errors=\"coerce\"\n",
        "            )\n",
        "            dataset = self.analyze_samples(dataset)\n",
        "\n",
        "            # Seleciona os últimos 7 dias mais completos de dados\n",
        "            daily_counts = dataset.groupby(dataset[\"timestamp\"].dt.date).size()\n",
        "            active_days = daily_counts[\n",
        "                daily_counts > 1000\n",
        "            ].index  # Dias com mais de 1000 interações\n",
        "            if len(active_days) > 7:\n",
        "                active_days = sorted(active_days)[-7:]  # Últimos 7 dias ativos\n",
        "                dataset = dataset[dataset[\"timestamp\"].dt.date.isin(active_days)]\n",
        "                print(\n",
        "                    f\"\\nSelecionados {len(active_days)} dias ativos com {len(dataset):,} registros\"\n",
        "                )\n",
        "\n",
        "            # Configura informações do dataset\n",
        "            self.history[\"dataset_info\"] = {\n",
        "                \"total_records\": len(dataset),\n",
        "                \"unique_users\": dataset[\"user_id\"].nunique(),\n",
        "                \"unique_articles\": dataset[\"click_article_id\"].nunique(),\n",
        "                \"start_time\": dataset[\"timestamp\"].min().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"end_time\": dataset[\"timestamp\"].max().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"window_size\": window_size,\n",
        "                \"step_size\": step_size,\n",
        "                \"total_days\": (\n",
        "                    dataset[\"timestamp\"].max() - dataset[\"timestamp\"].min()\n",
        "                ).total_seconds()\n",
        "                / (24 * 3600),\n",
        "            }\n",
        "\n",
        "            # Ordena o dataset pelo timestamp\n",
        "            dataset = dataset.sort_values(\"timestamp\")\n",
        "\n",
        "            # Converte timestamps para segundos desde o início\n",
        "            start_time = dataset[\"timestamp\"].min()\n",
        "            dataset[\"seconds\"] = (dataset[\"timestamp\"] - start_time).dt.total_seconds()\n",
        "\n",
        "            total_seconds = dataset[\"seconds\"].max()\n",
        "            window_starts = np.arange(0, total_seconds - window_size, step_size)\n",
        "\n",
        "            predictions = []\n",
        "            total_windows = len(window_starts)\n",
        "\n",
        "            for idx, window_start in enumerate(window_starts):\n",
        "                window_end = window_start + window_size\n",
        "\n",
        "                # Seleciona dados da janela atual\n",
        "                window_mask = (dataset[\"seconds\"] >= window_start) & (\n",
        "                    dataset[\"seconds\"] < window_end\n",
        "                )\n",
        "                window_data = dataset[window_mask]\n",
        "\n",
        "                print(f\"\\nProcessando janela {idx + 1}/{total_windows}\")\n",
        "                print(f\"Registros na janela: {len(window_data):,}\")\n",
        "\n",
        "                # Processa cada registro na janela\n",
        "                for _, row in window_data.iterrows():\n",
        "                    x = row.to_dict()\n",
        "\n",
        "                    # Treina o modelo\n",
        "                    self.model.learn_one(x)\n",
        "                    self.history[\"processed_records\"] += 1\n",
        "\n",
        "                    # Faz e armazena predição\n",
        "                    pred = self.model.predict_one(x)\n",
        "                    target = x[\"click_article_id\"]\n",
        "\n",
        "                    predictions.append(\n",
        "                        {\n",
        "                            \"timestamp\": x[\"timestamp\"],\n",
        "                            \"user_id\": x[\"user_id\"],\n",
        "                            \"click_article_id\": x[\"click_article_id\"],\n",
        "                            \"prediction\": pred,\n",
        "                            \"target\": x[\n",
        "                                \"click_article_id\"\n",
        "                            ],  # O alvo agora é o artigo atual\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                previous_row = x\n",
        "\n",
        "                # Report de progresso\n",
        "                if (idx + 1) % 10 == 0 or idx == len(window_starts) - 1:\n",
        "                    print(f\"\\nProgresso: {idx + 1}/{total_windows} janelas processadas\")\n",
        "                    print(\n",
        "                        f\"Registros processados: {self.history['processed_records']:,}\"\n",
        "                    )\n",
        "                    print(\"\\nMétricas atuais:\")\n",
        "                    print(\"\\nMétricas Random Forest:\")\n",
        "                    for name, metric in self.model.metrics[\"random_forest\"].items():\n",
        "                        print(f\"{name}: {metric.get()}\")\n",
        "                    print(\"\\nMétricas Baseline:\")\n",
        "                    for name, metric in self.model.metrics[\"popular_baseline\"].items():\n",
        "                        print(f\"{name}: {metric.get()}\")\n",
        "\n",
        "            # Salva todas as predições em um único arquivo\n",
        "            predictions_df = pd.DataFrame(predictions)\n",
        "            predictions_filename = f\"predictions_{self.timestamp}.csv\"\n",
        "            predictions_path = os.path.join(\n",
        "                \"/content/drive/MyDrive/Pesquisa2024/\", predictions_filename\n",
        "            )\n",
        "            predictions_df.to_csv(predictions_path, index=False)\n",
        "            print(f\"\\nPredições salvas em '{predictions_path}'\")\n",
        "\n",
        "            # Atualiza o histórico com o caminho das predições\n",
        "            self.history[\"predictions_file\"] = predictions_path\n",
        "\n",
        "            if output_model_path:\n",
        "                self.save_model_safely(output_model_path)\n",
        "\n",
        "            self._update_final_metrics()\n",
        "\n",
        "            print(\"\\nMétricas atuais:\")\n",
        "            print(\"\\nMétricas Random Forest:\")\n",
        "            for name, metric in self.model.metrics[\"random_forest\"].items():\n",
        "                print(f\"{name}: {metric.get():.4f}\")\n",
        "            print(\"\\nMétricas Baseline:\")\n",
        "            for name, metric in self.model.metrics[\"popular_baseline\"].items():\n",
        "                print(f\"{name}: {metric.get():.4f}\")\n",
        "\n",
        "            return self.history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro fatal processando dataset: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def save_model_safely(self, path):  # Adicionar self como primeiro parâmetro\n",
        "        \"\"\"Salva o modelo com verificação\"\"\"\n",
        "        try:\n",
        "            # Primeiro salva em um arquivo temporário\n",
        "            temp_path = path + \".temp\"\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                pickle.dump(self.model, f)  # Usar self.model ao invés de model\n",
        "\n",
        "            # Verifica se pode carregar\n",
        "            with open(temp_path, \"rb\") as f:\n",
        "                _ = pickle.load(f)\n",
        "\n",
        "            # Se chegou aqui, arquivo está ok\n",
        "            os.replace(temp_path, path)\n",
        "\n",
        "            # Atualizar informações no histórico\n",
        "            self.history[\"model_info\"][\"final_model_path\"] = path\n",
        "            self.history[\"model_info\"][\"save_time\"] = datetime.now().strftime(\n",
        "                \"%Y-%m-%d %H:%M:%S\"\n",
        "            )\n",
        "\n",
        "            print(f\"Modelo salvo com sucesso em {path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao salvar modelo: {str(e)}\")\n",
        "            if os.path.exists(temp_path):\n",
        "                os.remove(temp_path)\n",
        "            return False\n",
        "\n",
        "\n",
        "def main(\n",
        "    input_model: Optional[str] = None,\n",
        "    dataset_path: Optional[str] = None,\n",
        "    output_model: Optional[str] = None,\n",
        "    config: Optional[Dict[str, Any]] = None,\n",
        ") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Função principal para executar o sistema de recomendação.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not dataset_path:\n",
        "            raise ValueError(\"dataset_path é obrigatório\")\n",
        "\n",
        "        # Diretórios\n",
        "        base_dir = \"/content/drive/MyDrive/Pesquisa2024/\"\n",
        "        models_dir = os.path.join(base_dir, \"models\")\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        # Nome do modelo com timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        if not output_model:\n",
        "            output_model = os.path.join(models_dir, f\"news_recommender_{timestamp}.pkl\")\n",
        "\n",
        "        # Configuração padrão\n",
        "        config = config or {\"n_models\": 15, \"delta\": 0.002, \"top_k\": 10}\n",
        "\n",
        "        # Inicializa sistema\n",
        "        system = NewsRecommenderSystem(\n",
        "            model_path=input_model,\n",
        "            max_samples=None,\n",
        "            timestamp=timestamp,\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        # Carrega e processa dataset\n",
        "        print(f\"Carregando dataset de {dataset_path}...\")\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "        dataset[\"timestamp\"] = pd.to_datetime(dataset[\"click_timestamp\"], unit=\"ms\")\n",
        "\n",
        "        # Processa com janelas deslizantes (1 hora com passo de 30 minutos)\n",
        "        window_size = 3600  # 1 hora em segundos\n",
        "        step_size = 1800  # 30 minutos em segundos\n",
        "\n",
        "        history = system.train_and_predict_with_sliding_windows(\n",
        "            dataset=dataset,\n",
        "            window_size=window_size,\n",
        "            step_size=step_size,\n",
        "            output_model_path=output_model,\n",
        "        )\n",
        "\n",
        "        system.save_history()\n",
        "        return history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na execução principal: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "    # Uso do modelo existente:\n",
        "\n",
        "\n",
        "def continue_training(modelo_anterior_path, novos_dados_path, output_model_path):\n",
        "    \"\"\"\n",
        "    Continua o treinamento de um modelo existente\n",
        "    \"\"\"\n",
        "    # Carrega modelo anterior\n",
        "    with open(modelo_anterior_path, \"rb\") as f:\n",
        "        modelo = pickle.load(f)\n",
        "\n",
        "    # Carrega novos dados\n",
        "    novos_dados = pd.read_csv(novos_dados_path)\n",
        "\n",
        "    # Inicializa sistema com modelo existente\n",
        "    system = NewsRecommenderSystem(model_path=modelo_anterior_path, config=config)\n",
        "\n",
        "    # Processa novos dados\n",
        "    history = system.train_and_predict_with_sliding_windows(\n",
        "        dataset=novos_dados,\n",
        "        window_size=config[\"window_size\"],\n",
        "        step_size=config[\"step_size\"],\n",
        "        output_model_path=output_model_path,\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # treinamento\n",
        "    # configurações otimizadas\n",
        "    config = {\n",
        "        \"n_models\": 15,\n",
        "        \"delta\": 0.005,\n",
        "        \"top_k\": 10,\n",
        "        \"window_size\": 7200,  # 2 horas\n",
        "        \"step_size\": 3600,  # 1 hora\n",
        "        \"validation_ratio\": 0.3,\n",
        "    }\n",
        "\n",
        "    history = main(\n",
        "        dataset_path=\"/content/drive/MyDrive/Pesquisa2024/dataset_otimizado.csv\",\n",
        "        output_model=\"/content/drive/MyDrive/Pesquisa2024/models/modelo_recomendacoes1.pkl\",\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    # Carrega o modelo salvo\n",
        "    with open('/content/drive/MyDrive/Pesquisa2024/models/news_recommender_new-2.pkl', 'rb') as f:\n",
        "        modelo = pickle.load(f)\n",
        "\n",
        "    # Verifica métricas atuais\n",
        "    print(\"Métricas do modelo:\")\n",
        "    for nome, metrica in modelo.metrics.items():\n",
        "        print(f\"{nome}: {metrica.get():.4f}\")\n",
        "\n",
        "\n",
        "    # Continua treinamento com novos dados\n",
        "    history = continue_training(\n",
        "        modelo_anterior_path='/content/drive/MyDrive/Pesquisa2024/models/news_recommender_new-2.pkl',\n",
        "        novos_dados_path='/content/drive/MyDrive/Pesquisa2024/dataset_interacoes.csv',\n",
        "        output_model_path='/content/drive/MyDrive/Pesquisa2024/models/news_recommender_melhorado.pkl'\n",
        "    )\n",
        "    \"\"\"\n",
        "\n",
        "    if history:\n",
        "        print(\"\\nProcessamento concluído com sucesso!\")\n",
        "        print(\"\\n=== Estatísticas Finais ===\")\n",
        "        print(f\"Período total: {history['dataset_info']['total_days']:.1f} dias\")\n",
        "        print(f\"Total de registros: {history['metrics']['processed_records']:,}\")\n",
        "        print(f\"Tempo de processamento: {history['metrics']['processing_time']}\")\n",
        "\n",
        "        print(\"\\nMétricas Random Forest:\")\n",
        "        for metric_name, value in history[\"metrics\"][\"random_forest\"].items():\n",
        "            print(f\"{metric_name}: {value:.4f}\")\n",
        "\n",
        "        print(\"\\nMétricas Baseline:\")\n",
        "        for metric_name, value in history[\"metrics\"][\"popular_baseline\"].items():\n",
        "            print(f\"{metric_name}: {value:.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1OqdsrEOgoRURman5mfvCyRFgYj-tsfV2",
      "authorship_tag": "ABX9TyNAG67UmkkXjLCwliCDZ6s7",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}