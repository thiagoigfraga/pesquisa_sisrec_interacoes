{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMe1L6DdsCPH6bx/gg0sW/2"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_k5Lf3IxlS_",
        "outputId": "a2be8b99-6e6c-475e-adde-8466bed59e4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install river"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5wwn6239R0-",
        "outputId": "14decd31-2f21-4a9d-b488-0f85598dfca0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting river\n",
            "  Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from river) (2.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.1->river) (1.16.0)\n",
            "Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: river\n",
            "Successfully installed river-0.21.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 724
        },
        "id": "o8ElIu7HxeLL",
        "outputId": "f7dcac31-32af-48c8-8bd0-9041c9d85f87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando dataset de /content/drive/MyDrive/Pesquisa2024/dataset_interacoes.csv...\n",
            "\n",
            "Analisando amostras...\n",
            "\n",
            "=== Análise das Amostras ===\n",
            "Aviso: Encontradas 153,943 linhas duplicadas\n",
            "Removendo duplicatas...\n",
            "Aviso: Possível inconsistência na contagem de amostras\n",
            "Total de linhas: 2,988,182\n",
            "Total de interações únicas: 2,988,181\n",
            "Total de amostras: 2,988,181\n",
            "Período: 2017-10-01 03:00:00.026000 até 2017-11-13 20:04:14.886000\n",
            "Usuários únicos: 322,897\n",
            "Artigos únicos: 46,034\n",
            "\n",
            "Verificação de integridade:\n",
            "Valores nulos encontrados:\n",
            "click_timestamp    1\n",
            "timestamp          1\n",
            "dtype: int64\n",
            "\n",
            "Treinando com 500000 amostras\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-dcdb6ecefd27>\u001b[0m in \u001b[0;36m<cell line: 594>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    606\u001b[0m     \u001b[0;31m# Configuração\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 607\u001b[0;31m     history = main(\n\u001b[0m\u001b[1;32m    608\u001b[0m         \u001b[0mdataset_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/content/drive/MyDrive/Pesquisa2024/dataset_interacoes.csv'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    609\u001b[0m         \u001b[0mmax_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000000\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Controle de quantidade de amostras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dcdb6ecefd27>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(input_model, dataset_path, output_model, max_samples, train_size)\u001b[0m\n\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;31m# Processa dataset (treinamento e predição)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m         history = system.train_and_predict(\n\u001b[0m\u001b[1;32m    579\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0mtrain_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dcdb6ecefd27>\u001b[0m in \u001b[0;36mtrain_and_predict\u001b[0;34m(self, dataset, train_size, output_model_path)\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"\\nTreinando com {len(training_data)} amostras\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    420\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 421\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn_one\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    422\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'processed_records'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dcdb6ecefd27>\u001b[0m in \u001b[0;36mlearn_one\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# Extrai features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extract_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# Treina o modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-dcdb6ecefd27>\u001b[0m in \u001b[0;36m_extract_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mtotal_interactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marticle_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mtotal_users\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         features.update({\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from river import forest, metrics, drift, stats\n",
        "import random\n",
        "import hashlib\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import os\n",
        "import traceback\n",
        "from typing import Optional, Dict, Any\n",
        "from collections import Counter\n",
        "\n",
        "# Definir sementes para reprodutibilidade\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def convert_unix_timestamp(ts):\n",
        "    \"\"\"Converte timestamp Unix em milissegundos para datetime\"\"\"\n",
        "    return pd.to_datetime(ts, unit='ms')\n",
        "\n",
        "def consistent_hash(value):\n",
        "    \"\"\"Função de hash consistente usando SHA-256\"\"\"\n",
        "    return int(hashlib.sha256(str(value).encode('utf-8')).hexdigest(), 16) % 1000\n",
        "\n",
        "class OnlineNewsRecommender:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_models=10,\n",
        "        drift_detector=drift.ADWIN(),\n",
        "        top_k=10\n",
        "    ):\n",
        "        # Inicializa um normalizador para cada feature numérica\n",
        "        self.scalers = {\n",
        "            'user_id_hash': stats.Mean(),\n",
        "            'article_id_hash': stats.Mean(),\n",
        "            'hour_sin': stats.Mean(),\n",
        "            'hour_cos': stats.Mean(),\n",
        "            'day_sin': stats.Mean(),\n",
        "            'day_cos': stats.Mean(),\n",
        "            'month_sin': stats.Mean(),\n",
        "            'month_cos': stats.Mean(),\n",
        "            'hour': stats.Mean(),\n",
        "            'article_popularity': stats.Mean(),\n",
        "            'user_activity': stats.Mean()\n",
        "        }\n",
        "\n",
        "        self.vars = {k: stats.Var() for k in self.scalers.keys()}\n",
        "\n",
        "        # Modelo base com detector de drift\n",
        "        self.model = forest.ARFClassifier(\n",
        "            n_models=n_models,\n",
        "            drift_detector=drift_detector,\n",
        "            grace_period=50,\n",
        "            max_features='sqrt',\n",
        "            seed=42,\n",
        "            leaf_prediction='nb'\n",
        "        )\n",
        "\n",
        "        # Métricas online\n",
        "        self.metrics = {\n",
        "            'accuracy': metrics.Accuracy(),\n",
        "            'f1': metrics.F1(),\n",
        "            'precision': metrics.Precision(),\n",
        "            'recall': metrics.Recall(),\n",
        "            'roc_auc': metrics.ROCAUC(),\n",
        "            'log_loss': metrics.LogLoss()\n",
        "        }\n",
        "\n",
        "        # Contadores e estado\n",
        "        self.article_counter = Counter()\n",
        "        self.user_counter = Counter()\n",
        "        self.current_top = set()\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def _normalize_feature(self, name, value):\n",
        "        \"\"\"Normaliza uma feature usando média e variância online\"\"\"\n",
        "        # Atualiza estatísticas\n",
        "        self.scalers[name].update(value)\n",
        "        self.vars[name].update(value)\n",
        "\n",
        "        # Calcula z-score\n",
        "        mean = self.scalers[name].get()\n",
        "        var = self.vars[name].get()\n",
        "        std = np.sqrt(var) if var > 0 else 1\n",
        "\n",
        "        return (value - mean) / (std + 1e-8)\n",
        "\n",
        "    def _extract_features(self, x):\n",
        "        \"\"\"\n",
        "        Extrai as features do exemplo de entrada.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x : dict\n",
        "            Dicionário contendo os dados do exemplo atual.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Dicionário com as features normalizadas.\n",
        "        \"\"\"\n",
        "        timestamp = convert_unix_timestamp(x['click_timestamp'])\n",
        "\n",
        "        # Features básicas\n",
        "        features = {\n",
        "            'user_id_hash': consistent_hash(x['user_id']),\n",
        "            'article_id_hash': consistent_hash(x['click_article_id']),\n",
        "            'hour_sin': np.sin(2 * np.pi * timestamp.hour / 24),\n",
        "            'hour_cos': np.cos(2 * np.pi * timestamp.hour / 24),\n",
        "            'day_sin': np.sin(2 * np.pi * timestamp.dayofweek / 7),\n",
        "            'day_cos': np.cos(2 * np.pi * timestamp.dayofweek / 7),\n",
        "            'month_sin': np.sin(2 * np.pi * timestamp.month / 12),\n",
        "            'month_cos': np.cos(2 * np.pi * timestamp.month / 12),\n",
        "            'hour': timestamp.hour,\n",
        "        }\n",
        "\n",
        "        # Calcula popularidade excluindo a interação atual\n",
        "        article_count = self.article_counter.get(x['click_article_id'], 0)\n",
        "        user_count = self.user_counter.get(x['user_id'], 0)\n",
        "\n",
        "        total_interactions = sum(self.article_counter.values()) or 1\n",
        "        total_users = sum(self.user_counter.values()) or 1\n",
        "\n",
        "        features.update({\n",
        "            'article_popularity': article_count / total_interactions,\n",
        "            'user_activity': user_count / total_users,\n",
        "        })\n",
        "\n",
        "        # Normaliza features\n",
        "        normalized_features = {\n",
        "            name: self._normalize_feature(name, value)\n",
        "            for name, value in features.items()\n",
        "            if name in self.scalers\n",
        "        }\n",
        "\n",
        "        # Adiciona features categóricas sem normalização\n",
        "        normalized_features['weekend'] = int(timestamp.dayofweek >= 5)\n",
        "\n",
        "        return normalized_features\n",
        "\n",
        "    def learn_one(self, x):\n",
        "        \"\"\"Aprende com um exemplo\"\"\"\n",
        "        # Atualiza top articles antes de processar a interação atual para evitar vazamento\n",
        "        if sum(self.article_counter.values()) % 100 == 0:\n",
        "            self.current_top = set(\n",
        "                article for article, _ in\n",
        "                self.article_counter.most_common(self.top_k)\n",
        "            )\n",
        "\n",
        "        # Define target (1 se artigo está no top-k)\n",
        "        target = int(x['click_article_id'] in self.current_top)\n",
        "\n",
        "        # Extrai features\n",
        "        features = self._extract_features(x)\n",
        "\n",
        "        # Treina o modelo\n",
        "        self.model.learn_one(features, target)\n",
        "\n",
        "        # Atualiza contadores após definir o target\n",
        "        self.article_counter[x['click_article_id']] += 1\n",
        "        self.user_counter[x['user_id']] += 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        \"\"\"Faz predição para um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "        return self.model.predict_one(features)\n",
        "\n",
        "    def predict_proba_one(self, x):\n",
        "        \"\"\"Retorna probabilidades para um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "        return self.model.predict_proba_one(features)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Salva o modelo completo\"\"\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\"Carrega o modelo completo\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "class NewsRecommenderSystem:\n",
        "    \"\"\"Sistema para gerenciar treinamento e predição do recomendador de notícias.\"\"\"\n",
        "\n",
        "    def __init__(self, model_path=None, max_samples=None, timestamp=None, config=None):\n",
        "        \"\"\"\n",
        "        Inicializa o sistema de recomendação.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model_path : str, optional\n",
        "            Caminho para o modelo existente.\n",
        "        max_samples : int, optional\n",
        "            Número máximo de amostras a serem processadas.\n",
        "        timestamp : str, optional\n",
        "            Timestamp para nomear arquivos.\n",
        "        config : dict, optional\n",
        "            Configurações do modelo.\n",
        "        \"\"\"\n",
        "        self.model = None\n",
        "        self.history = {\n",
        "            'processed_records': 0,\n",
        "            'start_time': datetime.now(),\n",
        "            'metrics': {},\n",
        "            'dataset_info': {},\n",
        "            'model_info': {},\n",
        "            'config': config or {}\n",
        "        }\n",
        "        self.max_samples = max_samples\n",
        "        self.timestamp = timestamp or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        if model_path:\n",
        "            self._load_existing_model(model_path)\n",
        "        else:\n",
        "            self._initialize_new_model()\n",
        "\n",
        "    def _load_existing_model(self, model_path):\n",
        "        \"\"\"Carrega modelo existente.\"\"\"\n",
        "        try:\n",
        "            self.model = OnlineNewsRecommender.load(model_path)\n",
        "            self.history['model_info'] = {\n",
        "                'original_model_path': model_path,\n",
        "                'load_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Erro ao carregar modelo: {str(e)}\")\n",
        "\n",
        "    def _initialize_new_model(self):\n",
        "        \"\"\"Inicializa novo modelo.\"\"\"\n",
        "        n_models = self.history['config'].get('n_models', 15)\n",
        "        delta = self.history['config'].get('delta', 0.001)\n",
        "        top_k = self.history['config'].get('top_k', 10)\n",
        "\n",
        "        drift_detector = drift.ADWIN(delta=delta)\n",
        "\n",
        "        self.model = OnlineNewsRecommender(\n",
        "            n_models=n_models,\n",
        "            drift_detector=drift_detector,\n",
        "            top_k=top_k\n",
        "        )\n",
        "        # Salva as configurações do classificador\n",
        "        self.history['model_info'] = {\n",
        "            'initialization_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            'model_type': 'OnlineNewsRecommender',\n",
        "            'n_models': n_models,\n",
        "            'drift_detector': type(drift_detector).__name__,\n",
        "            'drift_detector_params': drift_detector.__dict__,\n",
        "            'top_k': top_k,\n",
        "            'model_params': {\n",
        "                'n_models': n_models,\n",
        "                'grace_period': self.model.model.grace_period,\n",
        "                'max_features': self.model.model.max_features,\n",
        "                'seed': self.model.model.seed,\n",
        "                'leaf_prediction': self.model.model.leaf_prediction,\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def validate_dataset(self, dataset: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        Valida se o dataset tem as colunas necessárias.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser validado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bool\n",
        "            True se o dataset é válido, levanta ValueError caso contrário.\n",
        "        \"\"\"\n",
        "        required_columns = ['user_id', 'click_article_id', 'click_timestamp']\n",
        "        missing_columns = [col for col in required_columns if col not in dataset.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Colunas faltando no dataset: {missing_columns}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _update_final_metrics(self):\n",
        "        \"\"\"\n",
        "        Atualiza métricas finais no histórico com informações adicionais.\n",
        "        \"\"\"\n",
        "        n_drifts = sum(self.model.model[n].drift_detector.n_detections\n",
        "                       for n in range(self.model.model.n_models))\n",
        "        n_warnings = sum(self.model.model[n].drift_detector.n_warnings\n",
        "                         for n in range(self.model.model.n_models))\n",
        "        top_articles = dict(self.model.article_counter.most_common(10))\n",
        "\n",
        "        self.history['metrics'] = {\n",
        "            'final_accuracy': self.model.metrics['accuracy'].get(),\n",
        "            'final_f1': self.model.metrics['f1'].get(),\n",
        "            'final_precision': self.model.metrics['precision'].get(),\n",
        "            'final_recall': self.model.metrics['recall'].get(),\n",
        "            'roc_auc': self.model.metrics['roc_auc'].get(),\n",
        "            'log_loss': self.model.metrics['log_loss'].get(),\n",
        "            'processed_records': self.history['processed_records'],\n",
        "            'processing_time': str(datetime.now() - self.history['start_time']),\n",
        "            'n_drifts': n_drifts,\n",
        "            'n_warnings': n_warnings,\n",
        "            'top_articles': str(top_articles),\n",
        "            'max_samples': self.max_samples\n",
        "        }\n",
        "\n",
        "    def analyze_samples(self, dataset):\n",
        "        \"\"\"\n",
        "        Analisa e mostra informações sobre as amostras do dataset com validações adicionais.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser analisado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            O dataset possivelmente limpo.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== Análise das Amostras ===\")\n",
        "\n",
        "            # Verifica duplicações\n",
        "            duplicates = dataset.duplicated().sum()\n",
        "            if duplicates > 0:\n",
        "                print(f\"Aviso: Encontradas {duplicates:,} linhas duplicadas\")\n",
        "                print(\"Removendo duplicatas...\")\n",
        "                dataset = dataset.drop_duplicates()\n",
        "\n",
        "            # Contagem de registros\n",
        "            total_samples = len(dataset)\n",
        "            total_interactions = dataset.groupby(['user_id', 'click_article_id', 'click_timestamp']).size().sum()\n",
        "\n",
        "            if total_samples != total_interactions:\n",
        "                print(f\"Aviso: Possível inconsistência na contagem de amostras\")\n",
        "                print(f\"Total de linhas: {total_samples:,}\")\n",
        "                print(f\"Total de interações únicas: {total_interactions:,}\")\n",
        "\n",
        "            print(f\"Total de amostras: {total_interactions:,}\")\n",
        "            print(f\"Período: {dataset['timestamp'].min()} até {dataset['timestamp'].max()}\")\n",
        "            print(f\"Usuários únicos: {dataset['user_id'].nunique():,}\")\n",
        "            print(f\"Artigos únicos: {dataset['click_article_id'].nunique():,}\")\n",
        "\n",
        "            # Verificar integridade dos dados\n",
        "            print(\"\\nVerificação de integridade:\")\n",
        "            null_counts = dataset.isnull().sum()\n",
        "            if null_counts.any():\n",
        "                print(\"Valores nulos encontrados:\")\n",
        "                print(null_counts[null_counts > 0])\n",
        "                # Tratar ou remover valores nulos conforme apropriado\n",
        "                dataset = dataset.dropna()\n",
        "\n",
        "            # Análise de timestamps\n",
        "            invalid_timestamps = pd.to_datetime(dataset['click_timestamp'], unit='ms', errors='coerce').isnull().sum()\n",
        "            if invalid_timestamps > 0:\n",
        "                print(f\"\\nAviso: {invalid_timestamps} timestamps inválidos encontrados\")\n",
        "                # Tratar timestamps inválidos\n",
        "                dataset = dataset.dropna(subset=['click_timestamp'])\n",
        "\n",
        "            # Análises adicionais podem ser adicionadas aqui...\n",
        "\n",
        "            return dataset  # Retorna dataset possivelmente limpo\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro analisando amostras: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return dataset\n",
        "\n",
        "    def train_and_predict(self, dataset, train_size=100000, output_model_path=None):\n",
        "        \"\"\"\n",
        "        Treina o modelo com uma quantidade definida de amostras e realiza predições em novas amostras.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser processado.\n",
        "        train_size : int\n",
        "            Número de amostras para treinamento.\n",
        "        output_model_path : str, optional\n",
        "            Caminho para salvar o modelo atualizado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            O histórico do processamento.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.validate_dataset(dataset)\n",
        "\n",
        "            # Análise das amostras\n",
        "            print(\"\\nAnalisando amostras...\")\n",
        "            dataset['timestamp'] = pd.to_datetime(\n",
        "                dataset['click_timestamp'],\n",
        "                unit='ms',\n",
        "                errors='coerce'\n",
        "            )\n",
        "            dataset = self.analyze_samples(dataset)\n",
        "\n",
        "            # Limita o número de amostras se max_samples for definido\n",
        "            if self.max_samples is not None:\n",
        "                dataset = dataset.head(self.max_samples)\n",
        "\n",
        "            self.history['dataset_info'] = {\n",
        "                'total_records': len(dataset),\n",
        "                'unique_users': dataset['user_id'].nunique(),\n",
        "                'unique_articles': dataset['click_article_id'].nunique(),\n",
        "                'start_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'train_size': train_size,\n",
        "                'test_size': len(dataset) - train_size,\n",
        "                'max_samples': self.max_samples\n",
        "            }\n",
        "\n",
        "            # Ordena o dataset pelo timestamp\n",
        "            dataset = dataset.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "            total_samples = len(dataset)\n",
        "\n",
        "            if train_size >= total_samples:\n",
        "                raise ValueError(\"train_size maior ou igual ao número total de amostras disponíveis.\")\n",
        "\n",
        "            # Dividir dataset em treinamento e teste\n",
        "            training_data = dataset.iloc[:train_size]\n",
        "            testing_data = dataset.iloc[train_size:]\n",
        "\n",
        "            # Treinamento\n",
        "            print(f\"\\nTreinando com {len(training_data)} amostras\")\n",
        "            for _, row in training_data.iterrows():\n",
        "                self.model.learn_one(row.to_dict())\n",
        "                self.history['processed_records'] += 1\n",
        "\n",
        "            # Predição\n",
        "            print(f\"\\nRealizando predições em {len(testing_data)} amostras\")\n",
        "            predictions = []\n",
        "            for _, row in testing_data.iterrows():\n",
        "                x = row.to_dict()\n",
        "                # Faz predição\n",
        "                pred = self.model.predict_one(x)\n",
        "                # Define target\n",
        "                target = int(x['click_article_id'] in self.model.current_top)\n",
        "                # Atualiza métricas\n",
        "                if pred is not None:\n",
        "                    for metric in self.model.metrics.values():\n",
        "                        metric.update(target, pred)\n",
        "                # Armazena predições\n",
        "                predictions.append({\n",
        "                    'user_id': x['user_id'],\n",
        "                    'click_article_id': x['click_article_id'],\n",
        "                    'prediction': pred,\n",
        "                    'target': target\n",
        "                })\n",
        "\n",
        "            # Salva todas as predições em um único arquivo\n",
        "            predictions_df = pd.DataFrame(predictions)\n",
        "            predictions_filename = f'predictions_{self.timestamp}.csv'\n",
        "            predictions_path = os.path.join('/content/drive/MyDrive/Pesquisa2024/', predictions_filename)\n",
        "            predictions_df.to_csv(predictions_path, index=False)\n",
        "            print(f\"Predições salvas em '{predictions_path}'\")\n",
        "\n",
        "            # Atualiza o histórico com o caminho das predições\n",
        "            self.history['predictions_file'] = predictions_path\n",
        "\n",
        "            if output_model_path:\n",
        "                self.save_model(output_model_path)\n",
        "\n",
        "            self._update_final_metrics()\n",
        "            return self.history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro fatal processando dataset: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Salva o modelo.\"\"\"\n",
        "        self.model.save(path)\n",
        "        self.history['model_info']['final_model_path'] = path\n",
        "        self.history['model_info']['save_time'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"\n",
        "        Salva o histórico do processamento em um arquivo CSV.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepara dados para o CSV\n",
        "            history_data = {\n",
        "                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'dataset_size': self.history['dataset_info'].get('total_records'),\n",
        "                'unique_users': self.history['dataset_info'].get('unique_users'),\n",
        "                'unique_articles': self.history['dataset_info'].get('unique_articles'),\n",
        "                'train_size': self.history['dataset_info'].get('train_size'),\n",
        "                'test_size': self.history['dataset_info'].get('test_size'),\n",
        "                'processed_records': self.history['metrics'].get('processed_records'),\n",
        "                'max_samples': self.history['dataset_info'].get('max_samples'),\n",
        "                'accuracy': self.history['metrics'].get('final_accuracy'),\n",
        "                'f1_score': self.history['metrics'].get('final_f1'),\n",
        "                'precision': self.history['metrics'].get('final_precision'),\n",
        "                'recall': self.history['metrics'].get('final_recall'),\n",
        "                'roc_auc': self.history['metrics'].get('roc_auc'),\n",
        "                'log_loss': self.history['metrics'].get('log_loss'),\n",
        "                'processing_time': self.history['metrics'].get('processing_time'),\n",
        "                'n_drifts': self.history['metrics'].get('n_drifts'),\n",
        "                'n_warnings': self.history['metrics'].get('n_warnings'),\n",
        "                'top_articles': self.history['metrics'].get('top_articles'),\n",
        "                'predictions_file': self.history.get('predictions_file'),\n",
        "                'model_path': self.history['model_info'].get('final_model_path'),\n",
        "                'original_model': self.history['model_info'].get('original_model_path'),\n",
        "                # Inclui as configurações do modelo\n",
        "                'model_type': self.history['model_info'].get('model_type'),\n",
        "                'n_models': self.history['model_info'].get('n_models'),\n",
        "                'delta': self.history['config'].get('delta'),\n",
        "                'drift_detector': self.history['model_info'].get('drift_detector'),\n",
        "                'drift_detector_params': str(self.history['model_info'].get('drift_detector_params')),\n",
        "                'top_k': self.history['model_info'].get('top_k'),\n",
        "                'model_params': str(self.history['model_info'].get('model_params'))\n",
        "            }\n",
        "\n",
        "            history_df = pd.DataFrame([history_data])\n",
        "\n",
        "            # Define arquivo apropriado\n",
        "            base_path = '/content/drive/MyDrive/Pesquisa2024/'\n",
        "            filename = 'training_history.csv'\n",
        "            filepath = os.path.join(base_path, filename)\n",
        "\n",
        "            # Garante que o diretório existe\n",
        "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "            # Verifica se arquivo existe\n",
        "            if os.path.exists(filepath):\n",
        "                existing_df = pd.read_csv(filepath)\n",
        "                history_df = pd.concat([existing_df, history_df], ignore_index=True)\n",
        "\n",
        "            history_df.to_csv(filepath, index=False)\n",
        "            print(f\"Histórico salvo em {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro salvando histórico: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "def main(input_model: Optional[str] = None,\n",
        "         dataset_path: Optional[str] = None,\n",
        "         output_model: Optional[str] = None,\n",
        "         max_samples: Optional[int] = None,\n",
        "         train_size: int = 100000,\n",
        "         config: Optional[Dict[str, Any]] = None) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Função principal para executar o sistema de recomendação.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    input_model : str, optional\n",
        "        Caminho do modelo existente.\n",
        "    dataset_path : str\n",
        "        Caminho do arquivo CSV com os dados.\n",
        "    output_model : str, optional\n",
        "        Caminho onde salvar o modelo atualizado.\n",
        "    max_samples : int, optional\n",
        "        Número máximo de amostras a serem processadas.\n",
        "    train_size : int, optional\n",
        "        Número de amostras para treinamento.\n",
        "    config : dict, optional\n",
        "        Configurações do modelo.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Histórico do processamento ou None em caso de erro.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not dataset_path:\n",
        "            raise ValueError(\"dataset_path é obrigatório\")\n",
        "\n",
        "        # Diretórios\n",
        "        base_dir = '/content/drive/MyDrive/Pesquisa2024/'\n",
        "        models_dir = os.path.join(base_dir, 'models')\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        # Nome do modelo com timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        output_model_name = f'news_recommender_{timestamp}.pkl'\n",
        "        output_model_path = output_model or os.path.join(models_dir, output_model_name)\n",
        "\n",
        "        # Inicializa sistema com as configurações\n",
        "        system = NewsRecommenderSystem(\n",
        "            model_path=input_model,\n",
        "            max_samples=max_samples,\n",
        "            timestamp=timestamp,\n",
        "            config=config\n",
        "        )\n",
        "\n",
        "        # Carrega dataset\n",
        "        print(f\"Carregando dataset de {dataset_path}...\")\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "        # Processa dataset (treinamento e predição)\n",
        "        history = system.train_and_predict(\n",
        "            dataset=dataset,\n",
        "            train_size=train_size,\n",
        "            output_model_path=output_model_path\n",
        "        )\n",
        "\n",
        "        # Salva histórico\n",
        "        system.save_history()\n",
        "\n",
        "        return history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na execução principal: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # Diretórios\n",
        "    base_dir = '/content/drive/MyDrive/Pesquisa2024/'\n",
        "    models_dir = os.path.join(base_dir, 'models')\n",
        "    os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "    # Nome do modelo com timestamp\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    output_model_name = f'news_recommender_{timestamp}.pkl'\n",
        "    output_model_path = os.path.join(models_dir, output_model_name)\n",
        "\n",
        "    # Configuração\n",
        "    config = {\n",
        "        'n_models': 15,\n",
        "        'delta': 0.001,\n",
        "        'top_k': 10\n",
        "    }\n",
        "\n",
        "    history = main(\n",
        "        dataset_path='/content/drive/MyDrive/Pesquisa2024/dataset_interacoes.csv',\n",
        "        max_samples=1500000,  # Controle de quantidade de amostras\n",
        "        train_size=100000,     # Número de amostras para treinamento\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    if history:\n",
        "        print(\"\\nProcessamento concluído com sucesso!\")\n",
        "\n",
        "        # Mostra estatísticas finais\n",
        "        print(\"\\n=== Estatísticas Finais ===\")\n",
        "        print(f\"Total de registros processados: {history['metrics']['processed_records']:,}\")\n",
        "        print(f\"Tempo total de processamento: {history['metrics']['processing_time']}\")\n",
        "\n",
        "        if history['metrics'].get('n_drifts'):\n",
        "            print(f\"Drifts detectados: {history['metrics']['n_drifts']}\")\n",
        "\n",
        "        # Mostra métricas finais\n",
        "        print(\"\\nMétricas Finais:\")\n",
        "        print(f\"Acurácia: {history['metrics']['final_accuracy']:.4f}\")\n",
        "        print(f\"F1 Score: {history['metrics']['final_f1']:.4f}\")\n",
        "        print(f\"Precisão: {history['metrics']['final_precision']:.4f}\")\n",
        "        print(f\"Recall: {history['metrics']['final_recall']:.4f}\")\n",
        "        print(f\"ROC AUC: {history['metrics']['roc_auc']:.4f}\")\n",
        "        print(f\"Log Loss: {history['metrics']['log_loss']:.4f}\")\n",
        "\n",
        "        # Mostra top artigos se disponível\n",
        "        if history['metrics'].get('top_articles'):\n",
        "            print(\"\\nTop Artigos:\")\n",
        "            top_articles = eval(history['metrics']['top_articles'])\n",
        "            for article, count in top_articles.items():\n",
        "                print(f\"Artigo {article}: {count:,} interações\")"
      ]
    }
  ]
}