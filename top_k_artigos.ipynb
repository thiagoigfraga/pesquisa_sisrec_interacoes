{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagoigfraga/pesquisa_sisrec_interacoes/blob/main/top_k_artigos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E_k5Lf3IxlS_",
        "outputId": "2e9302b5-3982-46e7-fb91-3a7e40f447ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5wwn6239R0-",
        "outputId": "93e2668e-a34f-4c60-c5c7-f85d3f3302ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting river\n",
            "  Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from river) (2.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.1->river) (1.16.0)\n",
            "Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/3.1 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/3.1 MB\u001b[0m \u001b[31m28.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m47.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: river\n",
            "Successfully installed river-0.21.2\n"
          ]
        }
      ],
      "source": [
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8ElIu7HxeLL",
        "outputId": "5afa0aba-6692-4659-e542-b9c3129e57f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando dataset de /content/drive/MyDrive/Pesquisa2024/dataset_otimizado.csv...\n",
            "\n",
            "=== Análise das Amostras ===\n",
            "Total de amostras: 2,987,469\n",
            "Período: 2017-10-01 03:00:00.026000 até 2017-10-17 23:51:27.187000\n",
            "Usuários únicos: 322,892\n",
            "Artigos únicos: 45,695\n",
            "\n",
            "Verificação de integridade:\n",
            "\n",
            "=== Análise Temporal ===\n",
            "\n",
            "Distribuição por hora do dia:\n",
            "hour\n",
            "0     126548\n",
            "1     120732\n",
            "2      94289\n",
            "3      61809\n",
            "4      32813\n",
            "5      18558\n",
            "6      14518\n",
            "7      16619\n",
            "8      32107\n",
            "9      72778\n",
            "10    129329\n",
            "11    184405\n",
            "12    161904\n",
            "13    168728\n",
            "14    182451\n",
            "15    187951\n",
            "16    201597\n",
            "17    191366\n",
            "18    190417\n",
            "19    192196\n",
            "20    179987\n",
            "21    152274\n",
            "22    141080\n",
            "23    133013\n",
            "dtype: int64\n",
            "\n",
            "Distribuição por dia:\n",
            "day\n",
            "2017-10-01     94056\n",
            "2017-10-02    303177\n",
            "2017-10-03    261159\n",
            "2017-10-04    215415\n",
            "2017-10-05    190003\n",
            "2017-10-06    207646\n",
            "2017-10-07    139323\n",
            "2017-10-08    108110\n",
            "2017-10-09    248208\n",
            "2017-10-10    282391\n",
            "2017-10-11    238969\n",
            "2017-10-12    121467\n",
            "2017-10-13    180723\n",
            "2017-10-14     95216\n",
            "2017-10-15     92163\n",
            "2017-10-16    189779\n",
            "2017-10-17     19664\n",
            "dtype: int64\n",
            "\n",
            "Intervalos entre interações (segundos):\n",
            "count    2.987468e+06\n",
            "mean     4.878670e-01\n",
            "std      1.723337e+01\n",
            "min      0.000000e+00\n",
            "25%      8.900000e-02\n",
            "50%      2.260000e-01\n",
            "75%      5.060000e-01\n",
            "max      2.942229e+04\n",
            "Name: time_diff, dtype: float64\n",
            "\n",
            "Validação Temporal:\n",
            "Dados de treino: 2,077,464 registros (11 dias)\n",
            "Dados de validação: 890,341 registros (6 dias)\n",
            "\n",
            "Treinando modelo...\n"
          ]
        }
      ],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from river import forest, metrics, drift, stats\n",
        "import random\n",
        "import hashlib\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "from collections import Counter, defaultdict\n",
        "import os\n",
        "import traceback\n",
        "from typing import Optional, Dict, Any\n",
        "\n",
        "# Definir sementes para reprodutibilidade\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "\n",
        "def convert_unix_timestamp(ts):\n",
        "    \"\"\"Converte timestamp Unix em milissegundos para datetime\"\"\"\n",
        "    return pd.to_datetime(ts, unit=\"ms\")\n",
        "\n",
        "\n",
        "def consistent_hash(value):\n",
        "    \"\"\"Função de hash consistente usando SHA-256\"\"\"\n",
        "    return (\n",
        "        int(hashlib.sha256(str(value).encode(\"utf-8\")).hexdigest(), 16) % 1000\n",
        "    )\n",
        "\n",
        "\n",
        "def analyze_temporal_distribution(dataset):\n",
        "    \"\"\"\n",
        "    Analisa a distribuição temporal dos dados\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Análise Temporal ===\")\n",
        "\n",
        "    # Converte timestamps\n",
        "    dataset[\"hour\"] = dataset[\"timestamp\"].dt.hour\n",
        "    dataset[\"day\"] = dataset[\"timestamp\"].dt.date\n",
        "\n",
        "    # Análise por hora\n",
        "    hourly_dist = dataset.groupby(\"hour\").size()\n",
        "    print(\"\\nDistribuição por hora do dia:\")\n",
        "    print(hourly_dist)\n",
        "\n",
        "    # Análise por dia\n",
        "    daily_dist = dataset.groupby(\"day\").size()\n",
        "    print(\"\\nDistribuição por dia:\")\n",
        "    print(daily_dist)\n",
        "\n",
        "    # Calcula intervalos entre interações\n",
        "    dataset = dataset.sort_values(\"timestamp\")\n",
        "    dataset[\"time_diff\"] = dataset[\"timestamp\"].diff().dt.total_seconds()\n",
        "\n",
        "    print(\"\\nIntervalos entre interações (segundos):\")\n",
        "    print(dataset[\"time_diff\"].describe())\n",
        "\n",
        "    return hourly_dist, daily_dist\n",
        "\n",
        "\n",
        "class OnlineNewsRecommender:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_models=15,  # Aumentado de 10 para 15\n",
        "        drift_detector=drift.ADWIN(delta=0.002),  # Ajustado delta\n",
        "        top_k=10,\n",
        "    ):\n",
        "        # Mantém histórico temporal\n",
        "        self.temporal_weights = {}  # Pesos temporais para artigos\n",
        "        self.article_timestamps = defaultdict(\n",
        "            list\n",
        "        )  # Últimos timestamps por artigo\n",
        "        self.time_window = 3600 * 6  # 6 horas em segundos\n",
        "\n",
        "        # Inicializa um normalizador para cada feature numérica\n",
        "        self.scalers = {\n",
        "            \"user_id_hash\": stats.Mean(),\n",
        "            \"article_id_hash\": stats.Mean(),\n",
        "            \"hour_sin\": stats.Mean(),\n",
        "            \"hour_cos\": stats.Mean(),\n",
        "            \"day_sin\": stats.Mean(),\n",
        "            \"day_cos\": stats.Mean(),\n",
        "            \"month_sin\": stats.Mean(),\n",
        "            \"month_cos\": stats.Mean(),\n",
        "            \"hour\": stats.Mean(),\n",
        "            \"article_popularity\": stats.Mean(),\n",
        "            \"user_activity\": stats.Mean(),\n",
        "            \"temporal_weight\": stats.Mean(),  # Adicionado\n",
        "        }\n",
        "\n",
        "        self.vars = {k: stats.Var() for k in self.scalers.keys()}\n",
        "\n",
        "        # Modelo base com detector de drift\n",
        "        self.model = forest.ARFClassifier(\n",
        "            n_models=n_models,\n",
        "            drift_detector=drift_detector,\n",
        "            grace_period=50,\n",
        "            max_features=\"sqrt\",\n",
        "            seed=42,\n",
        "            leaf_prediction=\"nb\",\n",
        "        )\n",
        "\n",
        "        # Métricas online\n",
        "        self.metrics = {\n",
        "            \"accuracy\": metrics.Accuracy(),\n",
        "            \"f1\": metrics.F1(),\n",
        "            \"precision\": metrics.Precision(),\n",
        "            \"recall\": metrics.Recall(),\n",
        "            \"roc_auc\": metrics.ROCAUC(),\n",
        "            \"log_loss\": metrics.LogLoss(),\n",
        "        }\n",
        "\n",
        "        # Contadores e estado\n",
        "        self.article_counter = Counter()\n",
        "        self.user_counter = Counter()\n",
        "        self.current_top = set()\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def _calculate_temporal_weight(self, article_id, current_timestamp):\n",
        "        \"\"\"\n",
        "        Calcula peso temporal do artigo baseado em suas interações recentes\n",
        "        \"\"\"\n",
        "        recent_timestamps = [\n",
        "            ts\n",
        "            for ts in self.article_timestamps[article_id]\n",
        "            if (current_timestamp - ts).total_seconds() <= self.time_window\n",
        "        ]\n",
        "\n",
        "        if not recent_timestamps:\n",
        "            return 0.0\n",
        "\n",
        "        # Peso decai exponencialmente com o tempo\n",
        "        weights = np.exp(\n",
        "            -0.1\n",
        "            * np.array(\n",
        "                [\n",
        "                    (current_timestamp - ts).total_seconds()\n",
        "                    / 3600  # Converte para horas\n",
        "                    for ts in recent_timestamps\n",
        "                ]\n",
        "            )\n",
        "        )\n",
        "\n",
        "        return np.mean(weights)\n",
        "\n",
        "    def _normalize_feature(self, name, value):\n",
        "        \"\"\"Normaliza uma feature usando média e variância online\"\"\"\n",
        "        # Atualiza estatísticas\n",
        "        self.scalers[name].update(value)\n",
        "        self.vars[name].update(value)\n",
        "\n",
        "        # Calcula z-score\n",
        "        mean = self.scalers[name].get()\n",
        "        var = self.vars[name].get()\n",
        "        std = np.sqrt(var) if var > 0 else 1\n",
        "\n",
        "        return (value - mean) / (std + 1e-8)\n",
        "\n",
        "    def _extract_features(self, x):\n",
        "        \"\"\"\n",
        "        Extrai as features do exemplo de entrada.\n",
        "        \"\"\"\n",
        "        timestamp = convert_unix_timestamp(x[\"click_timestamp\"])\n",
        "\n",
        "        # Features básicas\n",
        "        features = {\n",
        "            \"user_id_hash\": consistent_hash(x[\"user_id\"]),\n",
        "            \"article_id_hash\": consistent_hash(x[\"click_article_id\"]),\n",
        "            \"hour_sin\": np.sin(2 * np.pi * timestamp.hour / 24),\n",
        "            \"hour_cos\": np.cos(2 * np.pi * timestamp.hour / 24),\n",
        "            \"day_sin\": np.sin(2 * np.pi * timestamp.dayofweek / 7),\n",
        "            \"day_cos\": np.cos(2 * np.pi * timestamp.dayofweek / 7),\n",
        "            \"month_sin\": np.sin(2 * np.pi * timestamp.month / 12),\n",
        "            \"month_cos\": np.cos(2 * np.pi * timestamp.month / 12),\n",
        "            \"hour\": timestamp.hour,\n",
        "        }\n",
        "\n",
        "        # Calcula popularidade excluindo a interação atual\n",
        "        article_count = self.article_counter.get(x[\"click_article_id\"], 0)\n",
        "        user_count = self.user_counter.get(x[\"user_id\"], 0)\n",
        "\n",
        "        total_interactions = sum(self.article_counter.values()) or 1\n",
        "        total_users = sum(self.user_counter.values()) or 1\n",
        "\n",
        "        # Features temporais e de popularidade\n",
        "        temporal_weight = self._calculate_temporal_weight(\n",
        "            x[\"click_article_id\"], timestamp\n",
        "        )\n",
        "\n",
        "        features.update(\n",
        "            {\n",
        "                \"article_popularity\": article_count / total_interactions,\n",
        "                \"user_activity\": user_count / total_users,\n",
        "                \"temporal_weight\": temporal_weight,\n",
        "                \"is_business_hour\": int(9 <= timestamp.hour <= 18),\n",
        "                \"is_peak_hour\": int(10 <= timestamp.hour <= 16),\n",
        "                \"day_of_week\": timestamp.dayofweek,\n",
        "                \"is_weekend\": int(timestamp.dayofweek >= 5),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Normaliza features numéricas\n",
        "        normalized_features = {\n",
        "            name: self._normalize_feature(name, value)\n",
        "            for name, value in features.items()\n",
        "            if name in self.scalers\n",
        "        }\n",
        "\n",
        "        # Adiciona features categóricas sem normalização\n",
        "        normalized_features.update(\n",
        "            {\n",
        "                k: features[k]\n",
        "                for k in [\n",
        "                    \"is_business_hour\",\n",
        "                    \"is_peak_hour\",\n",
        "                    \"day_of_week\",\n",
        "                    \"is_weekend\",\n",
        "                ]\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # Atualiza histórico temporal\n",
        "        self.article_timestamps[x[\"click_article_id\"]].append(timestamp)\n",
        "        if (\n",
        "            len(self.article_timestamps[x[\"click_article_id\"]]) > 1000\n",
        "        ):  # Limita histórico\n",
        "            self.article_timestamps[x[\"click_article_id\"]] = (\n",
        "                self.article_timestamps[x[\"click_article_id\"]][-1000:]\n",
        "            )\n",
        "\n",
        "        return normalized_features\n",
        "\n",
        "    def learn_one(self, x):\n",
        "        \"\"\"Aprende com um exemplo\"\"\"\n",
        "        # Atualiza top articles antes de processar a interação atual para evitar vazamento\n",
        "        if sum(self.article_counter.values()) % 100 == 0:\n",
        "            self.current_top = set(\n",
        "                article\n",
        "                for article, _ in self.article_counter.most_common(self.top_k)\n",
        "            )\n",
        "\n",
        "        # Define target (1 se artigo está no top-k)\n",
        "        target = int(x[\"click_article_id\"] in self.current_top)\n",
        "\n",
        "        # Extrai features\n",
        "        features = self._extract_features(x)\n",
        "\n",
        "        # Treina o modelo\n",
        "        self.model.learn_one(features, target)\n",
        "\n",
        "        # Atualiza contadores após predição\n",
        "        self.article_counter[x[\"click_article_id\"]] += 1\n",
        "        self.user_counter[x[\"user_id\"]] += 1\n",
        "\n",
        "        # Atualiza métricas se houver predição\n",
        "        pred = self.model.predict_one(features)\n",
        "        if pred is not None:\n",
        "            for metric in self.metrics.values():\n",
        "                metric.update(target, pred)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        \"\"\"Faz predição para um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "        return self.model.predict_one(features)\n",
        "\n",
        "    def predict_proba_one(self, x):\n",
        "        \"\"\"Retorna probabilidades para um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "        return self.model.predict_proba_one(features)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Salva o modelo completo\"\"\"\n",
        "        with open(path, \"wb\") as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\"Carrega o modelo completo\"\"\"\n",
        "        with open(path, \"rb\") as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "\n",
        "class NewsRecommenderSystem:\n",
        "    \"\"\"Sistema para gerenciar treinamento e predição do recomendador de notícias.\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, model_path=None, max_samples=None, timestamp=None, config=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Inicializa o sistema de recomendação.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        model_path : str, optional\n",
        "            Caminho para o modelo existente.\n",
        "        max_samples : int, optional\n",
        "            Número máximo de amostras a serem processadas.\n",
        "        timestamp : str, optional\n",
        "            Timestamp para nomear arquivos.\n",
        "        config : dict, optional\n",
        "            Configurações do modelo.\n",
        "        \"\"\"\n",
        "        self.model = None\n",
        "        self.history = {\n",
        "            \"processed_records\": 0,\n",
        "            \"start_time\": datetime.now(),\n",
        "            \"metrics\": {},\n",
        "            \"dataset_info\": {},\n",
        "            \"model_info\": {},\n",
        "            \"config\": config or {},\n",
        "        }\n",
        "        self.max_samples = max_samples\n",
        "        self.timestamp = timestamp or datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "\n",
        "        if model_path:\n",
        "            self._load_existing_model(model_path)\n",
        "        else:\n",
        "            self._initialize_new_model()\n",
        "\n",
        "    def _load_existing_model(self, model_path):\n",
        "        \"\"\"Carrega modelo existente.\"\"\"\n",
        "        try:\n",
        "            self.model = OnlineNewsRecommender.load(model_path)\n",
        "            self.history[\"model_info\"] = {\n",
        "                \"original_model_path\": model_path,\n",
        "                \"load_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            }\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Erro ao carregar modelo: {str(e)}\")\n",
        "\n",
        "    def _initialize_new_model(self):\n",
        "        \"\"\"Inicializa novo modelo.\"\"\"\n",
        "        n_models = self.history[\"config\"].get(\"n_models\", 15)\n",
        "        delta = self.history[\"config\"].get(\"delta\", 0.002)  # Ajustado\n",
        "        top_k = self.history[\"config\"].get(\"top_k\", 10)\n",
        "\n",
        "        drift_detector = drift.ADWIN(delta=delta)\n",
        "\n",
        "        self.model = OnlineNewsRecommender(\n",
        "            n_models=n_models, drift_detector=drift_detector, top_k=top_k\n",
        "        )\n",
        "\n",
        "        # Salva as configurações do classificador\n",
        "        self.history[\"model_info\"] = {\n",
        "            \"initialization_time\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            \"model_type\": \"OnlineNewsRecommender\",\n",
        "            \"n_models\": n_models,\n",
        "            \"drift_detector\": type(drift_detector).__name__,\n",
        "            \"drift_detector_params\": drift_detector.__dict__,\n",
        "            \"top_k\": top_k,\n",
        "            \"model_params\": {\n",
        "                \"n_models\": n_models,\n",
        "                \"grace_period\": self.model.model.grace_period,\n",
        "                \"max_features\": self.model.model.max_features,\n",
        "                \"seed\": self.model.model.seed,\n",
        "                \"leaf_prediction\": self.model.model.leaf_prediction,\n",
        "            },\n",
        "        }\n",
        "\n",
        "    def validate_dataset(self, dataset: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        Valida se o dataset tem as colunas necessárias.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser validado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bool\n",
        "            True se o dataset é válido, levanta ValueError caso contrário.\n",
        "        \"\"\"\n",
        "        required_columns = [\"user_id\", \"click_article_id\", \"click_timestamp\"]\n",
        "        missing_columns = [\n",
        "            col for col in required_columns if col not in dataset.columns\n",
        "        ]\n",
        "\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Colunas faltando no dataset: {missing_columns}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _update_final_metrics(self):\n",
        "        \"\"\"\n",
        "        Atualiza métricas finais no histórico com informações adicionais.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Acessa drifts através do ARFClassifier\n",
        "            n_drifts = sum(\n",
        "                model.drift_detector.n_detections\n",
        "                for model in self.model.model._models\n",
        "                if hasattr(model, \"drift_detector\")\n",
        "            )\n",
        "            n_warnings = sum(\n",
        "                model.drift_detector.n_warnings\n",
        "                for model in self.model.model._models\n",
        "                if hasattr(model, \"drift_detector\")\n",
        "            )\n",
        "            top_articles = dict(self.model.article_counter.most_common(10))\n",
        "\n",
        "            self.history[\"metrics\"] = {\n",
        "                \"final_accuracy\": self.model.metrics[\"accuracy\"].get(),\n",
        "                \"final_f1\": self.model.metrics[\"f1\"].get(),\n",
        "                \"final_precision\": self.model.metrics[\"precision\"].get(),\n",
        "                \"final_recall\": self.model.metrics[\"recall\"].get(),\n",
        "                \"roc_auc\": self.model.metrics[\"roc_auc\"].get(),\n",
        "                \"log_loss\": self.model.metrics[\"log_loss\"].get(),\n",
        "                \"processed_records\": self.history[\"processed_records\"],\n",
        "                \"processing_time\": str(\n",
        "                    datetime.now() - self.history[\"start_time\"]\n",
        "                ),\n",
        "                \"n_drifts\": n_drifts,\n",
        "                \"n_warnings\": n_warnings,\n",
        "                \"top_articles\": str(top_articles),\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"Erro atualizando métricas: {str(e)}\")\n",
        "            self.history[\"metrics\"] = {\n",
        "                \"error\": str(e),\n",
        "                \"processed_records\": self.history[\"processed_records\"],\n",
        "                \"processing_time\": str(\n",
        "                    datetime.now() - self.history[\"start_time\"]\n",
        "                ),\n",
        "            }\n",
        "\n",
        "    def analyze_samples(self, dataset):\n",
        "        \"\"\"\n",
        "        Analisa e mostra informações sobre as amostras do dataset com validações adicionais.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== Análise das Amostras ===\")\n",
        "\n",
        "            # Verifica duplicações\n",
        "            duplicates = dataset.duplicated().sum()\n",
        "            if duplicates > 0:\n",
        "                print(f\"Aviso: Encontradas {duplicates:,} linhas duplicadas\")\n",
        "                print(\"Removendo duplicatas...\")\n",
        "                dataset = dataset.drop_duplicates()\n",
        "\n",
        "            # Contagem de registros\n",
        "            total_samples = len(dataset)\n",
        "            total_interactions = (\n",
        "                dataset.groupby(\n",
        "                    [\"user_id\", \"click_article_id\", \"click_timestamp\"]\n",
        "                )\n",
        "                .size()\n",
        "                .sum()\n",
        "            )\n",
        "\n",
        "            if total_samples != total_interactions:\n",
        "                print(f\"Aviso: Possível inconsistência na contagem de amostras\")\n",
        "                print(f\"Total de linhas: {total_samples:,}\")\n",
        "                print(f\"Total de interações únicas: {total_interactions:,}\")\n",
        "\n",
        "            print(f\"Total de amostras: {total_interactions:,}\")\n",
        "            print(\n",
        "                f\"Período: {dataset['timestamp'].min()} até {dataset['timestamp'].max()}\"\n",
        "            )\n",
        "            print(f\"Usuários únicos: {dataset['user_id'].nunique():,}\")\n",
        "            print(f\"Artigos únicos: {dataset['click_article_id'].nunique():,}\")\n",
        "\n",
        "            # Verificar integridade dos dados\n",
        "            print(\"\\nVerificação de integridade:\")\n",
        "            null_counts = dataset.isnull().sum()\n",
        "            if null_counts.any():\n",
        "                print(\"Valores nulos encontrados:\")\n",
        "                print(null_counts[null_counts > 0])\n",
        "                dataset = dataset.dropna()\n",
        "\n",
        "            # Análise de timestamps\n",
        "            invalid_timestamps = (\n",
        "                pd.to_datetime(\n",
        "                    dataset[\"click_timestamp\"], unit=\"ms\", errors=\"coerce\"\n",
        "                )\n",
        "                .isnull()\n",
        "                .sum()\n",
        "            )\n",
        "            if invalid_timestamps > 0:\n",
        "                print(\n",
        "                    f\"\\nAviso: {invalid_timestamps} timestamps inválidos encontrados\"\n",
        "                )\n",
        "                dataset = dataset.dropna(subset=[\"click_timestamp\"])\n",
        "\n",
        "            # Análise temporal\n",
        "            analyze_temporal_distribution(dataset)\n",
        "\n",
        "            return dataset\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro analisando amostras: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return dataset\n",
        "\n",
        "    def validate_temporal(self, dataset, validation_ratio=0.2):\n",
        "        \"\"\"\n",
        "        Realiza validação temporal do modelo usando uma proporção dos dados ativos\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            Dataset completo\n",
        "        validation_ratio : float\n",
        "            Proporção dos dados para validação (default: 0.2)\n",
        "        \"\"\"\n",
        "        # Identifica dias ativos (>1000 interações)\n",
        "        daily_counts = dataset.groupby(dataset[\"timestamp\"].dt.date).size()\n",
        "        active_days = daily_counts[daily_counts > 1000].index\n",
        "\n",
        "        # Seleciona apenas os primeiros 16 dias (período consistente)\n",
        "        active_days = sorted(active_days)[:16]\n",
        "\n",
        "        # Filtra dataset para usar apenas dias ativos\n",
        "        dataset = dataset[dataset[\"timestamp\"].dt.date.isin(active_days)]\n",
        "\n",
        "        # Ordena por timestamp\n",
        "        dataset = dataset.sort_values(\"timestamp\")\n",
        "\n",
        "        # Calcula ponto de corte baseado na proporção\n",
        "        n_samples = len(dataset)\n",
        "        split_idx = int(n_samples * (1 - validation_ratio))\n",
        "        split_time = dataset.iloc[split_idx][\"timestamp\"]\n",
        "\n",
        "        # Divide dados\n",
        "        train_data = dataset[dataset[\"timestamp\"] <= split_time]\n",
        "        val_data = dataset[dataset[\"timestamp\"] > split_time]\n",
        "\n",
        "        print(f\"\\nValidação Temporal:\")\n",
        "        print(\n",
        "            f\"Dados de treino: {len(train_data):,} registros \"\n",
        "            f\"({(split_time - dataset['timestamp'].min()).days + 1} dias)\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Dados de validação: {len(val_data):,} registros \"\n",
        "            f\"({(dataset['timestamp'].max() - split_time).days + 1} dias)\"\n",
        "        )\n",
        "\n",
        "        # Treina com dados de treino\n",
        "        print(\"\\nTreinando modelo...\")\n",
        "        for _, row in train_data.iterrows():\n",
        "            self.model.learn_one(row.to_dict())\n",
        "\n",
        "        # Valida com dados de validação\n",
        "        print(\"\\nValidando modelo...\")\n",
        "        val_metrics = {\n",
        "            \"accuracy\": metrics.Accuracy(),\n",
        "            \"f1\": metrics.F1(),\n",
        "            \"precision\": metrics.Precision(),\n",
        "            \"recall\": metrics.Recall(),\n",
        "            \"roc_auc\": metrics.ROCAUC(),\n",
        "            \"log_loss\": metrics.LogLoss(),\n",
        "        }\n",
        "\n",
        "        predictions = []\n",
        "        for _, row in val_data.iterrows():\n",
        "            x = row.to_dict()\n",
        "            pred = self.model.predict_one(x)\n",
        "            proba = self.model.predict_proba_one(x)\n",
        "            target = int(x[\"click_article_id\"] in self.model.current_top)\n",
        "\n",
        "            for name, metric in val_metrics.items():\n",
        "                if name in [\"roc_auc\", \"log_loss\"]:\n",
        "                    metric.update(target, proba)\n",
        "                else:\n",
        "                    metric.update(target, pred)\n",
        "\n",
        "            predictions.append(\n",
        "                {\n",
        "                    \"timestamp\": x[\"timestamp\"],\n",
        "                    \"user_id\": x[\"user_id\"],\n",
        "                    \"article_id\": x[\"click_article_id\"],\n",
        "                    \"prediction\": pred,\n",
        "                    \"probability\": proba.get(1, 0),\n",
        "                    \"target\": target,\n",
        "                }\n",
        "            )\n",
        "\n",
        "        print(\"\\nMétricas de Validação:\")\n",
        "        for name, metric in val_metrics.items():\n",
        "            print(f\"{name}: {metric.get():.4f}\")\n",
        "\n",
        "        return val_metrics, predictions\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"\n",
        "        Salva o histórico do processamento em um arquivo CSV.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            history_data = {\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"dataset_size\": self.history[\"dataset_info\"].get(\n",
        "                    \"total_records\"\n",
        "                ),\n",
        "                \"unique_users\": self.history[\"dataset_info\"].get(\n",
        "                    \"unique_users\"\n",
        "                ),\n",
        "                \"unique_articles\": self.history[\"dataset_info\"].get(\n",
        "                    \"unique_articles\"\n",
        "                ),\n",
        "                \"window_size\": self.history[\"dataset_info\"].get(\"window_size\"),\n",
        "                \"step_size\": self.history[\"dataset_info\"].get(\"step_size\"),\n",
        "                \"total_days\": self.history[\"dataset_info\"].get(\"total_days\"),\n",
        "                \"processed_records\": self.history[\"metrics\"].get(\n",
        "                    \"processed_records\"\n",
        "                ),\n",
        "                \"accuracy\": self.history[\"metrics\"].get(\"final_accuracy\"),\n",
        "                \"f1_score\": self.history[\"metrics\"].get(\"final_f1\"),\n",
        "                \"precision\": self.history[\"metrics\"].get(\"final_precision\"),\n",
        "                \"recall\": self.history[\"metrics\"].get(\"final_recall\"),\n",
        "                \"roc_auc\": self.history[\"metrics\"].get(\"roc_auc\"),\n",
        "                \"log_loss\": self.history[\"metrics\"].get(\"log_loss\"),\n",
        "                \"processing_time\": self.history[\"metrics\"].get(\n",
        "                    \"processing_time\"\n",
        "                ),\n",
        "                \"n_drifts\": self.history[\"metrics\"].get(\"n_drifts\"),\n",
        "                \"n_warnings\": self.history[\"metrics\"].get(\"n_warnings\"),\n",
        "                \"top_articles\": self.history[\"metrics\"].get(\"top_articles\"),\n",
        "                \"predictions_file\": self.history.get(\"predictions_file\"),\n",
        "                \"model_path\": self.history[\"model_info\"].get(\n",
        "                    \"final_model_path\"\n",
        "                ),\n",
        "                \"original_model\": self.history[\"model_info\"].get(\n",
        "                    \"original_model_path\"\n",
        "                ),\n",
        "                \"model_type\": self.history[\"model_info\"].get(\"model_type\"),\n",
        "                \"n_models\": self.history[\"model_info\"].get(\"n_models\"),\n",
        "                \"delta\": self.history[\"config\"].get(\"delta\"),\n",
        "                \"drift_detector\": self.history[\"model_info\"].get(\n",
        "                    \"drift_detector\"\n",
        "                ),\n",
        "                \"drift_detector_params\": str(\n",
        "                    self.history[\"model_info\"].get(\"drift_detector_params\")\n",
        "                ),\n",
        "                \"top_k\": self.history[\"model_info\"].get(\"top_k\"),\n",
        "                \"model_params\": str(\n",
        "                    self.history[\"model_info\"].get(\"model_params\")\n",
        "                ),\n",
        "            }\n",
        "\n",
        "            history_df = pd.DataFrame([history_data])\n",
        "            base_path = \"/content/drive/MyDrive/Pesquisa2024/\"\n",
        "            filename = \"training_history.csv\"\n",
        "            filepath = os.path.join(base_path, filename)\n",
        "\n",
        "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                existing_df = pd.read_csv(filepath)\n",
        "                history_df = pd.concat(\n",
        "                    [existing_df, history_df], ignore_index=True\n",
        "                )\n",
        "\n",
        "            history_df.to_csv(filepath, index=False)\n",
        "            print(f\"\\nHistórico salvo em {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro salvando histórico: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "    def train_and_predict_with_sliding_windows(\n",
        "        self, dataset, window_size=7200, step_size=3600, output_model_path=None\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Processa o dataset usando janelas deslizantes.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser processado.\n",
        "        window_size : int\n",
        "            Tamanho da janela em segundos (default: 1 hora)\n",
        "        step_size : int\n",
        "            Tamanho do passo em segundos (default: 30 minutos)\n",
        "        output_model_path : str, optional\n",
        "            Caminho para salvar o modelo atualizado.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.validate_dataset(dataset)\n",
        "            dataset[\"timestamp\"] = pd.to_datetime(\n",
        "                dataset[\"click_timestamp\"], unit=\"ms\"\n",
        "            )\n",
        "            dataset = self.analyze_samples(dataset)\n",
        "\n",
        "            # Realiza validação temporal primeiro\n",
        "            val_metrics, val_predictions = self.validate_temporal(\n",
        "                dataset=dataset.copy(),\n",
        "                validation_ratio=self.history[\"config\"].get(\n",
        "                    \"validation_ratio\", 0.2\n",
        "                ),\n",
        "            )\n",
        "\n",
        "            # Análise das amostras\n",
        "            print(\"\\nAnalisando amostras...\")\n",
        "            dataset[\"timestamp\"] = pd.to_datetime(\n",
        "                dataset[\"click_timestamp\"], unit=\"ms\", errors=\"coerce\"\n",
        "            )\n",
        "            dataset = self.analyze_samples(dataset)\n",
        "\n",
        "            # Seleciona os últimos 7 dias mais completos de dados\n",
        "            daily_counts = dataset.groupby(dataset[\"timestamp\"].dt.date).size()\n",
        "            active_days = daily_counts[\n",
        "                daily_counts > 1000\n",
        "            ].index  # Dias com mais de 1000 interações\n",
        "            if len(active_days) > 7:\n",
        "                active_days = sorted(active_days)[-7:]  # Últimos 7 dias ativos\n",
        "                dataset = dataset[\n",
        "                    dataset[\"timestamp\"].dt.date.isin(active_days)\n",
        "                ]\n",
        "                print(\n",
        "                    f\"\\nSelecionados {len(active_days)} dias ativos com {len(dataset):,} registros\"\n",
        "                )\n",
        "\n",
        "            # Configura informações do dataset\n",
        "            self.history[\"dataset_info\"] = {\n",
        "                \"total_records\": len(dataset),\n",
        "                \"unique_users\": dataset[\"user_id\"].nunique(),\n",
        "                \"unique_articles\": dataset[\"click_article_id\"].nunique(),\n",
        "                \"start_time\": dataset[\"timestamp\"]\n",
        "                .min()\n",
        "                .strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"end_time\": dataset[\"timestamp\"]\n",
        "                .max()\n",
        "                .strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"window_size\": window_size,\n",
        "                \"step_size\": step_size,\n",
        "                \"total_days\": (\n",
        "                    dataset[\"timestamp\"].max() - dataset[\"timestamp\"].min()\n",
        "                ).total_seconds()\n",
        "                / (24 * 3600),\n",
        "            }\n",
        "\n",
        "            # Ordena o dataset pelo timestamp\n",
        "            dataset = dataset.sort_values(\"timestamp\")\n",
        "\n",
        "            # Converte timestamps para segundos desde o início\n",
        "            start_time = dataset[\"timestamp\"].min()\n",
        "            dataset[\"seconds\"] = (\n",
        "                dataset[\"timestamp\"] - start_time\n",
        "            ).dt.total_seconds()\n",
        "\n",
        "            total_seconds = dataset[\"seconds\"].max()\n",
        "            window_starts = np.arange(0, total_seconds - window_size, step_size)\n",
        "\n",
        "            predictions = []\n",
        "            total_windows = len(window_starts)\n",
        "\n",
        "            for idx, window_start in enumerate(window_starts):\n",
        "                window_end = window_start + window_size\n",
        "\n",
        "                # Seleciona dados da janela atual\n",
        "                window_mask = (dataset[\"seconds\"] >= window_start) & (\n",
        "                    dataset[\"seconds\"] < window_end\n",
        "                )\n",
        "                window_data = dataset[window_mask]\n",
        "\n",
        "                print(f\"\\nProcessando janela {idx + 1}/{total_windows}\")\n",
        "                print(f\"Registros na janela: {len(window_data):,}\")\n",
        "\n",
        "                # Processa cada registro na janela\n",
        "                for _, row in window_data.iterrows():\n",
        "                    x = row.to_dict()\n",
        "\n",
        "                    # Treina o modelo\n",
        "                    self.model.learn_one(x)\n",
        "                    self.history[\"processed_records\"] += 1\n",
        "\n",
        "                    # Faz e armazena predição\n",
        "                    pred = self.model.predict_one(x)\n",
        "                    target = int(\n",
        "                        x[\"click_article_id\"] in self.model.current_top\n",
        "                    )\n",
        "\n",
        "                    predictions.append(\n",
        "                        {\n",
        "                            \"timestamp\": x[\"timestamp\"],\n",
        "                            \"user_id\": x[\"user_id\"],\n",
        "                            \"click_article_id\": x[\"click_article_id\"],\n",
        "                            \"prediction\": pred,\n",
        "                            \"target\": target,\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                # Report de progresso\n",
        "                if (idx + 1) % 10 == 0 or idx == len(window_starts) - 1:\n",
        "                    print(\n",
        "                        f\"\\nProgresso: {idx + 1}/{total_windows} janelas processadas\"\n",
        "                    )\n",
        "                    print(\n",
        "                        f\"Registros processados: {self.history['processed_records']:,}\"\n",
        "                    )\n",
        "                    print(\"\\nMétricas atuais:\")\n",
        "                    for name, metric in self.model.metrics.items():\n",
        "                        print(f\"{name}: {metric.get():.4f}\")\n",
        "\n",
        "            # Salva todas as predições em um único arquivo\n",
        "            predictions_df = pd.DataFrame(predictions)\n",
        "            predictions_filename = f\"predictions_{self.timestamp}.csv\"\n",
        "            predictions_path = os.path.join(\n",
        "                \"/content/drive/MyDrive/Pesquisa2024/\", predictions_filename\n",
        "            )\n",
        "            predictions_df.to_csv(predictions_path, index=False)\n",
        "            print(f\"\\nPredições salvas em '{predictions_path}'\")\n",
        "\n",
        "            # Atualiza o histórico com o caminho das predições\n",
        "            self.history[\"predictions_file\"] = predictions_path\n",
        "\n",
        "            if output_model_path:\n",
        "                self.save_model(output_model_path)\n",
        "\n",
        "            self._update_final_metrics()\n",
        "\n",
        "            self.history[\"validation_metrics\"] = {\n",
        "                name: metric.get() for name, metric in val_metrics.items()\n",
        "            }\n",
        "\n",
        "            return self.history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro fatal processando dataset: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def save_model_safely(model, path):\n",
        "        \"\"\"\n",
        "        Salva o modelo com verificação\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Primeiro salva em um arquivo temporário\n",
        "            temp_path = path + \".temp\"\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                pickle.dump(model, f)\n",
        "\n",
        "            # Verifica se pode carregar\n",
        "            with open(temp_path, \"rb\") as f:\n",
        "                _ = pickle.load(f)\n",
        "\n",
        "            # Se chegou aqui, arquivo está ok\n",
        "            os.replace(temp_path, path)\n",
        "            print(f\"Modelo salvo com sucesso em {path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro ao salvar modelo: {str(e)}\")\n",
        "            if os.path.exists(temp_path):\n",
        "                os.remove(temp_path)\n",
        "            return False\n",
        "\n",
        "    '''\n",
        "    def save_model(self, path):\n",
        "        \"\"\"\n",
        "        Salva o modelo com verificação de segurança\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        path : str\n",
        "            Caminho onde o modelo será salvo\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Primeiro salva em um arquivo temporário\n",
        "            temp_path = path + \".temp\"\n",
        "\n",
        "            # Salva o modelo em arquivo temporário\n",
        "            with open(temp_path, \"wb\") as f:\n",
        "                pickle.dump(self.model, f)\n",
        "\n",
        "            # Verifica se pode carregar\n",
        "            with open(temp_path, \"rb\") as f:\n",
        "                _ = pickle.load(f)\n",
        "\n",
        "            # Se chegou aqui, arquivo está ok\n",
        "            os.replace(temp_path, path)\n",
        "\n",
        "            # Atualiza informações no histórico\n",
        "            self.history[\"model_info\"][\"final_model_path\"] = path\n",
        "            self.history[\"model_info\"][\"save_time\"] = datetime.now().strftime(\n",
        "                \"%Y-%m-%d %H:%M:%S\"\n",
        "            )\n",
        "\n",
        "            print(f\"\\nModelo salvo com sucesso em {path}\")\n",
        "            return True\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nErro ao salvar modelo: {str(e)}\")\n",
        "            if os.path.exists(temp_path):\n",
        "                os.remove(temp_path)\n",
        "            return False\n",
        "            '''\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"\n",
        "        Salva o histórico do processamento em um arquivo CSV.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            history_data = {\n",
        "                \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                \"dataset_size\": self.history[\"dataset_info\"].get(\n",
        "                    \"total_records\"\n",
        "                ),\n",
        "                \"unique_users\": self.history[\"dataset_info\"].get(\n",
        "                    \"unique_users\"\n",
        "                ),\n",
        "                \"unique_articles\": self.history[\"dataset_info\"].get(\n",
        "                    \"unique_articles\"\n",
        "                ),\n",
        "                \"window_size\": self.history[\"dataset_info\"].get(\"window_size\"),\n",
        "                \"step_size\": self.history[\"dataset_info\"].get(\"step_size\"),\n",
        "                \"total_days\": self.history[\"dataset_info\"].get(\"total_days\"),\n",
        "                \"processed_records\": self.history[\"metrics\"].get(\n",
        "                    \"processed_records\"\n",
        "                ),\n",
        "                \"accuracy\": self.history[\"metrics\"].get(\"final_accuracy\"),\n",
        "                \"f1_score\": self.history[\"metrics\"].get(\"final_f1\"),\n",
        "                \"precision\": self.history[\"metrics\"].get(\"final_precision\"),\n",
        "                \"recall\": self.history[\"metrics\"].get(\"final_recall\"),\n",
        "                \"roc_auc\": self.history[\"metrics\"].get(\"roc_auc\"),\n",
        "                \"log_loss\": self.history[\"metrics\"].get(\"log_loss\"),\n",
        "                \"processing_time\": self.history[\"metrics\"].get(\n",
        "                    \"processing_time\"\n",
        "                ),\n",
        "                \"n_drifts\": self.history[\"metrics\"].get(\"n_drifts\"),\n",
        "                \"n_warnings\": self.history[\"metrics\"].get(\"n_warnings\"),\n",
        "                \"top_articles\": self.history[\"metrics\"].get(\"top_articles\"),\n",
        "                \"predictions_file\": self.history.get(\"predictions_file\"),\n",
        "                \"model_path\": self.history[\"model_info\"].get(\n",
        "                    \"final_model_path\"\n",
        "                ),\n",
        "                \"original_model\": self.history[\"model_info\"].get(\n",
        "                    \"original_model_path\"\n",
        "                ),\n",
        "                \"model_type\": self.history[\"model_info\"].get(\"model_type\"),\n",
        "                \"n_models\": self.history[\"model_info\"].get(\"n_models\"),\n",
        "                \"delta\": self.history[\"config\"].get(\"delta\"),\n",
        "                \"drift_detector\": self.history[\"model_info\"].get(\n",
        "                    \"drift_detector\"\n",
        "                ),\n",
        "                \"drift_detector_params\": str(\n",
        "                    self.history[\"model_info\"].get(\"drift_detector_params\")\n",
        "                ),\n",
        "                \"top_k\": self.history[\"model_info\"].get(\"top_k\"),\n",
        "                \"model_params\": str(\n",
        "                    self.history[\"model_info\"].get(\"model_params\")\n",
        "                ),\n",
        "            }\n",
        "\n",
        "            history_df = pd.DataFrame([history_data])\n",
        "            base_path = \"/content/drive/MyDrive/Pesquisa2024/\"\n",
        "            filename = \"training_history.csv\"\n",
        "            filepath = os.path.join(base_path, filename)\n",
        "\n",
        "            os.makedirs(os.path.dirname(filepath), exist_ok=True)\n",
        "\n",
        "            if os.path.exists(filepath):\n",
        "                existing_df = pd.read_csv(filepath)\n",
        "                history_df = pd.concat(\n",
        "                    [existing_df, history_df], ignore_index=True\n",
        "                )\n",
        "\n",
        "            history_df.to_csv(filepath, index=False)\n",
        "            print(f\"\\nHistórico salvo em {filepath}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro salvando histórico: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "\n",
        "def main(\n",
        "    input_model: Optional[str] = None,\n",
        "    dataset_path: Optional[str] = None,\n",
        "    output_model: Optional[str] = None,\n",
        "    config: Optional[Dict[str, Any]] = None,\n",
        ") -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Função principal para executar o sistema de recomendação.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not dataset_path:\n",
        "            raise ValueError(\"dataset_path é obrigatório\")\n",
        "\n",
        "        # Diretórios\n",
        "        base_dir = \"/content/drive/MyDrive/Pesquisa2024/\"\n",
        "        models_dir = os.path.join(base_dir, \"models\")\n",
        "        os.makedirs(models_dir, exist_ok=True)\n",
        "\n",
        "        # Nome do modelo com timestamp\n",
        "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        if not output_model:\n",
        "            output_model = os.path.join(\n",
        "                models_dir, f\"news_recommender_{timestamp}.pkl\"\n",
        "            )\n",
        "\n",
        "        # Configuração padrão\n",
        "        config = config or {\"n_models\": 15, \"delta\": 0.002, \"top_k\": 10}\n",
        "\n",
        "        # Inicializa sistema\n",
        "        system = NewsRecommenderSystem(\n",
        "            model_path=input_model,\n",
        "            max_samples=None,\n",
        "            timestamp=timestamp,\n",
        "            config=config,\n",
        "        )\n",
        "\n",
        "        # Carrega e processa dataset\n",
        "        print(f\"Carregando dataset de {dataset_path}...\")\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "        dataset[\"timestamp\"] = pd.to_datetime(\n",
        "            dataset[\"click_timestamp\"], unit=\"ms\"\n",
        "        )\n",
        "\n",
        "        # Processa com janelas deslizantes (1 hora com passo de 30 minutos)\n",
        "        window_size = 3600  # 1 hora em segundos\n",
        "        step_size = 1800  # 30 minutos em segundos\n",
        "\n",
        "        history = system.train_and_predict_with_sliding_windows(\n",
        "            dataset=dataset,\n",
        "            window_size=window_size,\n",
        "            step_size=step_size,\n",
        "            output_model_path=output_model,\n",
        "        )\n",
        "\n",
        "        system.save_history()\n",
        "        return history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na execução principal: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "    # Uso do modelo existente:\n",
        "\n",
        "\n",
        "def continue_training(\n",
        "    modelo_anterior_path, novos_dados_path, output_model_path\n",
        "):\n",
        "    \"\"\"\n",
        "    Continua o treinamento de um modelo existente\n",
        "    \"\"\"\n",
        "    # Carrega modelo anterior\n",
        "    with open(modelo_anterior_path, \"rb\") as f:\n",
        "        modelo = pickle.load(f)\n",
        "\n",
        "    # Carrega novos dados\n",
        "    novos_dados = pd.read_csv(novos_dados_path)\n",
        "\n",
        "    # Inicializa sistema com modelo existente\n",
        "    system = NewsRecommenderSystem(\n",
        "        model_path=modelo_anterior_path, config=config\n",
        "    )\n",
        "\n",
        "    # Processa novos dados\n",
        "    history = system.train_and_predict_with_sliding_windows(\n",
        "        dataset=novos_dados,\n",
        "        window_size=config[\"window_size\"],\n",
        "        step_size=config[\"step_size\"],\n",
        "        output_model_path=output_model_path,\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    # treinamento\n",
        "    # configurações otimizadas\n",
        "    config = {\n",
        "        \"n_models\": 15,\n",
        "        \"delta\": 0.005,\n",
        "        \"top_k\": 10,\n",
        "        \"window_size\": 7200,  # 2 horas\n",
        "        \"step_size\": 3600,  # 1 hora\n",
        "        \"validation_ratio\": 0.3,\n",
        "    }\n",
        "\n",
        "    history = main(\n",
        "        dataset_path=\"/content/drive/MyDrive/Pesquisa2024/dataset_otimizado.csv\",\n",
        "        output_model=\"/content/drive/MyDrive/Pesquisa2024/models/modelo_recomendacoes1.pkl\",\n",
        "        config=config,\n",
        "    )\n",
        "\n",
        "    \"\"\"\n",
        "    # Carrega o modelo salvo\n",
        "    with open('/content/drive/MyDrive/Pesquisa2024/models/news_recommender_new-2.pkl', 'rb') as f:\n",
        "        modelo = pickle.load(f)\n",
        "\n",
        "    # Verifica métricas atuais\n",
        "    print(\"Métricas do modelo:\")\n",
        "    for nome, metrica in modelo.metrics.items():\n",
        "        print(f\"{nome}: {metrica.get():.4f}\")\n",
        "\n",
        "\n",
        "    # Continua treinamento com novos dados\n",
        "    history = continue_training(\n",
        "        modelo_anterior_path='/content/drive/MyDrive/Pesquisa2024/models/news_recommender_new-2.pkl',\n",
        "        novos_dados_path='/content/drive/MyDrive/Pesquisa2024/dataset_interacoes.csv',\n",
        "        output_model_path='/content/drive/MyDrive/Pesquisa2024/models/news_recommender_melhorado.pkl'\n",
        "    )\n",
        "    \"\"\"\n",
        "\n",
        "    if history:\n",
        "        print(\"\\nProcessamento concluído com sucesso!\")\n",
        "        print(\"\\n=== Estatísticas Finais ===\")\n",
        "        print(\n",
        "            f\"Período total: {history['dataset_info']['total_days']:.1f} dias\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Total de registros: {history['metrics']['processed_records']:,}\"\n",
        "        )\n",
        "        print(\n",
        "            f\"Tempo de processamento: {history['metrics']['processing_time']}\"\n",
        "        )\n",
        "\n",
        "        print(\"\\nMétricas:\")\n",
        "        print(f\"Acurácia: {history['metrics']['final_accuracy']:.4f}\")\n",
        "        print(f\"F1 Score: {history['metrics']['final_f1']:.4f}\")\n",
        "        print(f\"Precisão: {history['metrics']['final_precision']:.4f}\")\n",
        "        print(f\"Recall: {history['metrics']['final_recall']:.4f}\")\n",
        "        print(f\"ROC AUC: {history['metrics']['roc_auc']:.4f}\")\n",
        "        print(f\"Log Loss: {history['metrics']['log_loss']:.4f}\")\n",
        "\n",
        "        print(f\"\\nDrifts detectados: {history['metrics']['n_drifts']}\")\n",
        "        print(f\"Warnings detectados: {history['metrics']['n_warnings']}\")\n",
        "\n",
        "        if history[\"metrics\"].get(\"top_articles\"):\n",
        "            print(\"\\nTop Artigos:\")\n",
        "            top_articles = eval(history[\"metrics\"][\"top_articles\"])\n",
        "            for article, count in top_articles.items():\n",
        "                print(f\"Artigo {article}: {count:,} interações\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1OqdsrEOgoRURman5mfvCyRFgYj-tsfV2",
      "authorship_tag": "ABX9TyMXCY0NRGzZJ/ip0qp6IyKH",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}