{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/thiagoigfraga/pesquisa_sisrec_interacoes/blob/main/top_k_artigos.pynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXob54tbnxDN",
        "outputId": "6437832f-b724-4a4d-e2a1-3c2a50b1b208"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6C0i7m0eHaf1",
        "outputId": "98aa05ce-38f3-4966-deee-a000fc7d16df"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting river\n",
            "  Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in /usr/local/lib/python3.10/dist-packages (from river) (1.26.4)\n",
            "Requirement already satisfied: pandas<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from river) (2.2.2)\n",
            "Requirement already satisfied: scipy<2.0.0,>=1.12.1 in /usr/local/lib/python3.10/dist-packages (from river) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=2.1->river) (2024.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=2.1->river) (1.16.0)\n",
            "Downloading river-0.21.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: river\n",
            "Successfully installed river-0.21.2\n"
          ]
        }
      ],
      "source": [
        "!pip install river"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_J2IoaOyHUlv",
        "outputId": "5c5b966f-4c8a-4da1-c5c3-b70eda8a1bec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Carregando dataset de /content/drive/MyDrive/Pesquisa2024/dataset_interacoes.csv...\n",
            "\n",
            "Analisando amostras...\n",
            "\n",
            "=== Análise das Amostras ===\n",
            "Aviso: Encontradas 153,943 linhas duplicadas\n",
            "Removendo duplicatas...\n",
            "Aviso: Possível inconsistência na contagem de amostras\n",
            "Total de linhas: 2,988,182\n",
            "Total de interações únicas: 2,988,181\n",
            "Total de amostras: 2,988,181\n",
            "Período: 2017-10-01 03:00:00.026000 até 2017-11-13 20:04:14.886000\n",
            "Usuários únicos: 322,897\n",
            "Artigos únicos: 46,034\n",
            "\n",
            "Verificação de integridade:\n",
            "Valores nulos encontrados:\n",
            "click_timestamp    1\n",
            "timestamp          1\n",
            "dtype: int64\n",
            "\n",
            "Processados 50000 exemplos\n",
            "\n",
            "Processados 100000 exemplos\n",
            "\n",
            "Processados 150000 exemplos\n",
            "\n",
            "Processados 200000 exemplos\n",
            "\n",
            "Processados 250000 exemplos\n",
            "\n",
            "Processados 300000 exemplos\n",
            "\n",
            "Processados 350000 exemplos\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from river import forest, metrics, drift, stats\n",
        "import random\n",
        "import hashlib\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "import os\n",
        "import traceback\n",
        "from typing import Optional, Dict, Any\n",
        "from collections import Counter\n",
        "\n",
        "# Definir sementes para reprodutibilidade\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "\n",
        "def convert_unix_timestamp(ts):\n",
        "    \"\"\"Converte timestamp Unix em milissegundos para datetime\"\"\"\n",
        "    return pd.to_datetime(ts, unit='ms')\n",
        "\n",
        "def consistent_hash(value):\n",
        "    \"\"\"Função de hash consistente usando SHA-256\"\"\"\n",
        "    return int(hashlib.sha256(str(value).encode('utf-8')).hexdigest(), 16) % 1000\n",
        "\n",
        "class OnlineNewsRecommender:\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_models=10,\n",
        "        drift_detector=drift.ADWIN(),\n",
        "        top_k=10\n",
        "    ):\n",
        "        # Inicializa um normalizador para cada feature numérica\n",
        "        self.scalers = {\n",
        "            'user_id_hash': stats.Mean(),\n",
        "            'article_id_hash': stats.Mean(),\n",
        "            'hour_sin': stats.Mean(),\n",
        "            'hour_cos': stats.Mean(),\n",
        "            'day_sin': stats.Mean(),\n",
        "            'day_cos': stats.Mean(),\n",
        "            'month_sin': stats.Mean(),\n",
        "            'month_cos': stats.Mean(),\n",
        "            'hour': stats.Mean(),\n",
        "            'article_popularity': stats.Mean(),\n",
        "            'user_activity': stats.Mean()\n",
        "        }\n",
        "\n",
        "        self.vars = {k: stats.Var() for k in self.scalers.keys()}\n",
        "\n",
        "        # Modelo base com detector de drift\n",
        "        self.model = forest.ARFClassifier(\n",
        "            n_models=n_models,\n",
        "            drift_detector=drift_detector,\n",
        "            grace_period=50,\n",
        "            max_features='sqrt',\n",
        "            seed=42,\n",
        "            leaf_prediction='nb'\n",
        "        )\n",
        "\n",
        "        # Métricas online\n",
        "        self.metrics = {\n",
        "            'accuracy': metrics.Accuracy(),\n",
        "            'f1': metrics.F1(),\n",
        "            'precision': metrics.Precision(),\n",
        "            'recall': metrics.Recall()\n",
        "        }\n",
        "\n",
        "        # Contadores e estado\n",
        "        self.article_counter = Counter()\n",
        "        self.user_counter = Counter()\n",
        "        self.current_top = set()\n",
        "        self.top_k = top_k\n",
        "\n",
        "    def _normalize_feature(self, name, value):\n",
        "        \"\"\"Normaliza uma feature usando média e variância online\"\"\"\n",
        "        # Atualiza estatísticas\n",
        "        self.scalers[name].update(value)\n",
        "        self.vars[name].update(value)\n",
        "\n",
        "        # Calcula z-score\n",
        "        mean = self.scalers[name].get()\n",
        "        std = np.sqrt(self.vars[name].get()) if self.vars[name].get() > 0 else 1\n",
        "\n",
        "        return (value - mean) / (std + 1e-8)\n",
        "\n",
        "    def _extract_features(self, x):\n",
        "        \"\"\"\n",
        "        Extrai as features do exemplo de entrada.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        x : dict\n",
        "            Dicionário contendo os dados do exemplo atual.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            Dicionário com as features normalizadas.\n",
        "        \"\"\"\n",
        "        timestamp = convert_unix_timestamp(x['click_timestamp'])\n",
        "\n",
        "        # Features básicas\n",
        "        features = {\n",
        "            'user_id_hash': consistent_hash(x['user_id']),\n",
        "            'article_id_hash': consistent_hash(x['click_article_id']),\n",
        "            'hour_sin': np.sin(2 * np.pi * timestamp.hour / 24),\n",
        "            'hour_cos': np.cos(2 * np.pi * timestamp.hour / 24),\n",
        "            'day_sin': np.sin(2 * np.pi * timestamp.dayofweek / 7),\n",
        "            'day_cos': np.cos(2 * np.pi * timestamp.dayofweek / 7),\n",
        "            'month_sin': np.sin(2 * np.pi * timestamp.month / 12),\n",
        "            'month_cos': np.cos(2 * np.pi * timestamp.month / 12),\n",
        "            'hour': timestamp.hour,\n",
        "        }\n",
        "\n",
        "        # Calcula popularidade excluindo a interação atual\n",
        "        article_count = self.article_counter.get(x['click_article_id'], 0)\n",
        "        user_count = self.user_counter.get(x['user_id'], 0)\n",
        "\n",
        "        total_interactions = sum(self.article_counter.values()) or 1\n",
        "        total_users = sum(self.user_counter.values()) or 1\n",
        "\n",
        "        features.update({\n",
        "            'article_popularity': article_count / total_interactions,\n",
        "            'user_activity': user_count / total_users,\n",
        "        })\n",
        "\n",
        "        # Normaliza features\n",
        "        normalized_features = {\n",
        "            name: self._normalize_feature(name, value)\n",
        "            for name, value in features.items()\n",
        "            if name in self.scalers\n",
        "        }\n",
        "\n",
        "        # Adiciona features categóricas sem normalização\n",
        "        normalized_features['weekend'] = int(timestamp.dayofweek >= 5)\n",
        "\n",
        "        return normalized_features\n",
        "\n",
        "    def learn_one(self, x):\n",
        "        \"\"\"Aprende com um exemplo\"\"\"\n",
        "        # Atualiza top articles antes de processar a interação atual para evitar vazamento\n",
        "        if sum(self.article_counter.values()) % 100 == 0:\n",
        "            self.current_top = set(\n",
        "                article for article, _ in\n",
        "                self.article_counter.most_common(self.top_k)\n",
        "            )\n",
        "\n",
        "        # Define target (1 se artigo está no top-k)\n",
        "        target = int(x['click_article_id'] in self.current_top)\n",
        "\n",
        "        # Atualiza contadores após definir o target\n",
        "        self.article_counter[x['click_article_id']] += 1\n",
        "        self.user_counter[x['user_id']] += 1\n",
        "\n",
        "        # Extrai features\n",
        "        features = self._extract_features(x)\n",
        "\n",
        "        # Treina o modelo\n",
        "        self.model.learn_one(features, target)\n",
        "\n",
        "        # Atualiza métricas\n",
        "        pred = self.model.predict_one(features)\n",
        "        if pred is not None:\n",
        "            for metric in self.metrics.values():\n",
        "                metric.update(target, pred)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def predict_one(self, x):\n",
        "        \"\"\"Faz predição para um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "        return self.model.predict_one(features)\n",
        "\n",
        "    def predict_proba_one(self, x):\n",
        "        \"\"\"Retorna probabilidades para um exemplo\"\"\"\n",
        "        features = self._extract_features(x)\n",
        "        return self.model.predict_proba_one(features)\n",
        "\n",
        "    def save(self, path):\n",
        "        \"\"\"Salva o modelo completo\"\"\"\n",
        "        with open(path, 'wb') as f:\n",
        "            pickle.dump(self, f)\n",
        "\n",
        "    @classmethod\n",
        "    def load(cls, path):\n",
        "        \"\"\"Carrega o modelo completo\"\"\"\n",
        "        with open(path, 'rb') as f:\n",
        "            return pickle.load(f)\n",
        "\n",
        "\n",
        "class NewsRecommenderSystem:\n",
        "    \"\"\"Sistema para gerenciar treinamento e predição do recomendador de notícias.\"\"\"\n",
        "\n",
        "    def __init__(self, mode='train', model_path=None):\n",
        "        \"\"\"\n",
        "        Inicializa o sistema de recomendação.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        mode : str\n",
        "            'train' para treinar novo modelo, 'predict' para usar modelo existente.\n",
        "        model_path : str, optional\n",
        "            Caminho para o modelo existente quando mode='predict'.\n",
        "        \"\"\"\n",
        "        self.mode = mode\n",
        "        self.model = None\n",
        "        self.history = {\n",
        "            'processed_records': 0,\n",
        "            'start_time': datetime.now(),\n",
        "            'metrics': {},\n",
        "            'dataset_info': {},\n",
        "            'model_info': {}\n",
        "        }\n",
        "\n",
        "        if mode == 'predict' and model_path:\n",
        "            self._load_existing_model(model_path)\n",
        "        else:\n",
        "            self._initialize_new_model()\n",
        "\n",
        "    def _load_existing_model(self, model_path):\n",
        "        \"\"\"Carrega modelo existente para predição.\"\"\"\n",
        "        try:\n",
        "            self.model = OnlineNewsRecommender.load(model_path)\n",
        "            self.history['model_info'] = {\n",
        "                'original_model_path': model_path,\n",
        "                'load_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "            }\n",
        "        except Exception as e:\n",
        "            raise Exception(f\"Erro ao carregar modelo: {str(e)}\")\n",
        "\n",
        "    def _initialize_new_model(self):\n",
        "        \"\"\"Inicializa novo modelo para treinamento.\"\"\"\n",
        "        self.model = OnlineNewsRecommender(\n",
        "            n_models=15,\n",
        "            drift_detector=drift.ADWIN(delta=0.001),\n",
        "            top_k=10\n",
        "        )\n",
        "        self.history['model_info'] = {\n",
        "            'initialization_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "            'model_type': 'OnlineNewsRecommender',\n",
        "            'n_models': 15,\n",
        "            'drift_detector': 'ADWIN',\n",
        "            'top_k': 10\n",
        "        }\n",
        "\n",
        "    def validate_dataset(self, dataset: pd.DataFrame) -> bool:\n",
        "        \"\"\"\n",
        "        Valida se o dataset tem as colunas necessárias.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser validado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        bool\n",
        "            True se o dataset é válido, levanta ValueError caso contrário.\n",
        "        \"\"\"\n",
        "        required_columns = ['user_id', 'click_article_id', 'click_timestamp']\n",
        "        missing_columns = [col for col in required_columns if col not in dataset.columns]\n",
        "\n",
        "        if missing_columns:\n",
        "            raise ValueError(f\"Colunas faltando no dataset: {missing_columns}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def _update_final_metrics(self):\n",
        "        \"\"\"\n",
        "        Atualiza métricas finais no histórico com informações adicionais.\n",
        "        \"\"\"\n",
        "        if self.mode == 'train':\n",
        "            n_drifts = sum(self.model.model[n].drift_detector.n_detections\n",
        "                           for n in range(self.model.model.n_models))\n",
        "            n_warnings = sum(self.model.model[n].drift_detector.n_warnings\n",
        "                             for n in range(self.model.model.n_models))\n",
        "            top_articles = dict(self.model.article_counter.most_common(10))\n",
        "        else:\n",
        "            n_drifts = n_warnings = None\n",
        "            top_articles = None\n",
        "\n",
        "        self.history['metrics'] = {\n",
        "            'final_accuracy': self.model.metrics['accuracy'].get() if self.mode == 'train' else None,\n",
        "            'final_f1': self.model.metrics['f1'].get() if self.mode == 'train' else None,\n",
        "            'final_precision': self.model.metrics['precision'].get() if self.mode == 'train' else None,\n",
        "            'final_recall': self.model.metrics['recall'].get() if self.mode == 'train' else None,\n",
        "            'processed_records': self.history['processed_records'],\n",
        "            'processing_time': str(datetime.now() - self.history['start_time']),\n",
        "            'n_drifts': n_drifts,\n",
        "            'n_warnings': n_warnings,\n",
        "            'top_articles': str(top_articles) if top_articles else None\n",
        "        }\n",
        "\n",
        "    def analyze_samples(self, dataset):\n",
        "        \"\"\"\n",
        "        Analisa e mostra informações sobre as amostras do dataset com validações adicionais.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser analisado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        pd.DataFrame\n",
        "            O dataset possivelmente limpo.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            print(\"\\n=== Análise das Amostras ===\")\n",
        "\n",
        "            # Verifica duplicações\n",
        "            duplicates = dataset.duplicated().sum()\n",
        "            if duplicates > 0:\n",
        "                print(f\"Aviso: Encontradas {duplicates:,} linhas duplicadas\")\n",
        "                print(\"Removendo duplicatas...\")\n",
        "                dataset = dataset.drop_duplicates()\n",
        "\n",
        "            # Contagem de registros\n",
        "            total_samples = len(dataset)\n",
        "            total_interactions = dataset.groupby(['user_id', 'click_article_id', 'click_timestamp']).size().sum()\n",
        "\n",
        "            if total_samples != total_interactions:\n",
        "                print(f\"Aviso: Possível inconsistência na contagem de amostras\")\n",
        "                print(f\"Total de linhas: {total_samples:,}\")\n",
        "                print(f\"Total de interações únicas: {total_interactions:,}\")\n",
        "\n",
        "            print(f\"Total de amostras: {total_interactions:,}\")\n",
        "            print(f\"Período: {dataset['timestamp'].min()} até {dataset['timestamp'].max()}\")\n",
        "            print(f\"Usuários únicos: {dataset['user_id'].nunique():,}\")\n",
        "            print(f\"Artigos únicos: {dataset['click_article_id'].nunique():,}\")\n",
        "\n",
        "            # Verificar integridade dos dados\n",
        "            print(\"\\nVerificação de integridade:\")\n",
        "            null_counts = dataset.isnull().sum()\n",
        "            if null_counts.any():\n",
        "                print(\"Valores nulos encontrados:\")\n",
        "                print(null_counts[null_counts > 0])\n",
        "                # Tratar ou remover valores nulos conforme apropriado\n",
        "                dataset = dataset.dropna()\n",
        "\n",
        "            # Análise de timestamps\n",
        "            invalid_timestamps = pd.to_datetime(dataset['click_timestamp'], unit='ms', errors='coerce').isnull().sum()\n",
        "            if invalid_timestamps > 0:\n",
        "                print(f\"\\nAviso: {invalid_timestamps} timestamps inválidos encontrados\")\n",
        "                # Tratar timestamps inválidos\n",
        "                dataset = dataset.dropna(subset=['click_timestamp'])\n",
        "\n",
        "            # Análises adicionais podem ser adicionadas aqui...\n",
        "\n",
        "            return dataset  # Retorna dataset possivelmente limpo\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro analisando amostras: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return dataset\n",
        "\n",
        "    def process_dataset(self, dataset, batch_size=1000, report_interval=10000, output_model_path=None):\n",
        "        \"\"\"\n",
        "        Processa o dataset, treinando ou prevendo com base no modo.\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        dataset : pd.DataFrame\n",
        "            O dataset a ser processado.\n",
        "        batch_size : int\n",
        "            Número de registros a processar por lote.\n",
        "        report_interval : int\n",
        "            Intervalo para reportar progresso.\n",
        "        output_model_path : str, optional\n",
        "            Caminho para salvar o modelo atualizado.\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        dict\n",
        "            O histórico do processamento.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            self.validate_dataset(dataset)\n",
        "\n",
        "            # Análise das amostras\n",
        "            print(\"\\nAnalisando amostras...\")\n",
        "            dataset['timestamp'] = pd.to_datetime(\n",
        "                dataset['click_timestamp'],\n",
        "                unit='ms',\n",
        "                errors='coerce'\n",
        "            )\n",
        "            dataset = self.analyze_samples(dataset)\n",
        "\n",
        "            self.history['dataset_info'] = {\n",
        "                'total_records': len(dataset),\n",
        "                'unique_users': dataset['user_id'].nunique(),\n",
        "                'unique_articles': dataset['click_article_id'].nunique(),\n",
        "                'start_time': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'batch_size': batch_size,\n",
        "                'report_interval': report_interval\n",
        "            }\n",
        "\n",
        "            # Ordena o dataset pelo timestamp\n",
        "            dataset = dataset.sort_values('timestamp').reset_index(drop=True)\n",
        "\n",
        "            # Inicializa lista de predições se em modo de predição\n",
        "            predictions = []\n",
        "\n",
        "            for start_idx in range(0, len(dataset), batch_size):\n",
        "                try:\n",
        "                    batch = dataset.iloc[start_idx:start_idx + batch_size]\n",
        "\n",
        "                    for _, row in batch.iterrows():\n",
        "                        if self.mode == 'train':\n",
        "                            # Aprende com o exemplo\n",
        "                            self.model.learn_one(row.to_dict())\n",
        "                        else:\n",
        "                            # Faz predição e armazena\n",
        "                            prediction = self.model.predict_one(row.to_dict())\n",
        "                            predictions.append({\n",
        "                                'user_id': row['user_id'],\n",
        "                                'click_article_id': row['click_article_id'],\n",
        "                                'prediction': prediction\n",
        "                            })\n",
        "                        self.history['processed_records'] += 1\n",
        "\n",
        "                    if (start_idx + batch_size) % report_interval == 0:\n",
        "                        self._report_progress()\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Erro processando batch {start_idx}-{start_idx+batch_size}: {str(e)}\")\n",
        "                    traceback.print_exc()\n",
        "                    continue\n",
        "\n",
        "            if self.mode == 'predict' and predictions:\n",
        "                predictions_df = pd.DataFrame(predictions)\n",
        "                predictions_df.to_csv('predictions.csv', index=False)\n",
        "                print(\"Predições salvas em 'predictions.csv'\")\n",
        "\n",
        "            if output_model_path:\n",
        "                self.save_model(output_model_path)\n",
        "\n",
        "            self._update_final_metrics()\n",
        "            return self.history\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro fatal processando dataset: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "    def _report_progress(self):\n",
        "        \"\"\"Reporta progresso do processamento.\"\"\"\n",
        "        print(f\"\\nProcessados {self.history['processed_records']} exemplos\")\n",
        "\n",
        "        if self.mode == 'train':\n",
        "            print(\"\\nMétricas atuais:\")\n",
        "            for name, metric in self.model.metrics.items():\n",
        "                print(f\"{name}: {metric.get():.4f}\")\n",
        "\n",
        "            n_drifts = sum(self.model.model[n].drift_detector.n_detections\n",
        "                           for n in range(self.model.model.n_models))\n",
        "            print(f\"\\nDrifts detectados: {n_drifts}\")\n",
        "\n",
        "    def save_model(self, path):\n",
        "        \"\"\"Salva o modelo.\"\"\"\n",
        "        self.model.save(path)\n",
        "        self.history['model_info']['final_model_path'] = path\n",
        "        self.history['model_info']['save_time'] = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
        "\n",
        "    def save_history(self):\n",
        "        \"\"\"\n",
        "        Versão melhorada do salvamento de histórico.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepara dados para o CSV\n",
        "            history_df = pd.DataFrame([{\n",
        "                'timestamp': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
        "                'mode': self.mode,\n",
        "                'dataset_size': self.history['dataset_info'].get('total_records'),\n",
        "                'unique_users': self.history['dataset_info'].get('unique_users'),\n",
        "                'unique_articles': self.history['dataset_info'].get('unique_articles'),\n",
        "                'batch_size': self.history['dataset_info'].get('batch_size'),\n",
        "                'processed_records': self.history['metrics'].get('processed_records'),\n",
        "                'accuracy': self.history['metrics'].get('final_accuracy'),\n",
        "                'f1_score': self.history['metrics'].get('final_f1'),\n",
        "                'precision': self.history['metrics'].get('final_precision'),\n",
        "                'recall': self.history['metrics'].get('final_recall'),\n",
        "                'processing_time': self.history['metrics'].get('processing_time'),\n",
        "                'n_drifts': self.history['metrics'].get('n_drifts'),\n",
        "                'n_warnings': self.history['metrics'].get('n_warnings'),\n",
        "                'top_articles': self.history['metrics'].get('top_articles'),\n",
        "                'model_path': self.history['model_info'].get('final_model_path'),\n",
        "                'original_model': self.history['model_info'].get('original_model_path')\n",
        "            }])\n",
        "\n",
        "            # Define arquivo apropriado\n",
        "            filename = 'training_history.csv' if self.mode == 'train' else 'prediction_history.csv'\n",
        "\n",
        "            # Garante que o diretório existe\n",
        "            os.makedirs(os.path.dirname(filename) if os.path.dirname(filename) else '.', exist_ok=True)\n",
        "\n",
        "            # Verifica se arquivo existe\n",
        "            if os.path.exists(filename):\n",
        "                existing_df = pd.read_csv(filename)\n",
        "                history_df = pd.concat([existing_df, history_df], ignore_index=True)\n",
        "\n",
        "            history_df.to_csv(filename, index=False)\n",
        "            print(f\"Histórico salvo em {filename}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Erro salvando histórico: {str(e)}\")\n",
        "            traceback.print_exc()\n",
        "\n",
        "def main(mode: str = 'train',\n",
        "         input_model: Optional[str] = None,\n",
        "         dataset_path: Optional[str] = None,\n",
        "         output_model: Optional[str] = None) -> Optional[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Função principal para executar o sistema de recomendação.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    mode : str\n",
        "        Modo de operação: 'train' ou 'predict'.\n",
        "    input_model : str, optional\n",
        "        Caminho do modelo para modo 'predict'.\n",
        "    dataset_path : str\n",
        "        Caminho do arquivo CSV com os dados.\n",
        "    output_model : str, optional\n",
        "        Caminho onde salvar o modelo atualizado.\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    dict\n",
        "        Histórico do processamento ou None em caso de erro.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Validações\n",
        "        if mode not in ['train', 'predict']:\n",
        "            raise ValueError(\"Mode deve ser 'train' ou 'predict'\")\n",
        "\n",
        "        if mode == 'predict' and not input_model:\n",
        "            raise ValueError(\"input_model é obrigatório no modo 'predict'\")\n",
        "\n",
        "        if not dataset_path:\n",
        "            raise ValueError(\"dataset_path é obrigatório\")\n",
        "\n",
        "        # Inicializa sistema\n",
        "        system = NewsRecommenderSystem(mode=mode, model_path=input_model)\n",
        "\n",
        "        # Carrega dataset\n",
        "        print(f\"Carregando dataset de {dataset_path}...\")\n",
        "        dataset = pd.read_csv(dataset_path)\n",
        "\n",
        "        # Processa dataset\n",
        "        history = system.process_dataset(\n",
        "            dataset=dataset,\n",
        "            batch_size=1000,\n",
        "            report_interval=50000,\n",
        "            output_model_path=output_model\n",
        "        )\n",
        "\n",
        "        # Salva histórico\n",
        "        system.save_history()\n",
        "\n",
        "        return history\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro na execução principal: {str(e)}\")\n",
        "        traceback.print_exc()\n",
        "        return None\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Configuração exemplo\n",
        "    history = main(\n",
        "        mode='predict',\n",
        "        input_model='/content/drive/MyDrive/Pesquisa2024/models/news_recommender_new2.pkl',\n",
        "        dataset_path='/content/drive/MyDrive/Pesquisa2024/dataset_interacoes.csv',\n",
        "        output_model='news_recommender_new3.pkl'\n",
        "    )\n",
        "\n",
        "    # Uso para treinamento\n",
        "    '''\n",
        "    main(\n",
        "        mode='train',\n",
        "        dataset_path='dataset_treino.csv',\n",
        "        output_model='modelo_treinado.pkl'\n",
        "    )\n",
        "    '''\n",
        "\n",
        "    if history:\n",
        "        print(\"\\nProcessamento concluído com sucesso!\")\n",
        "\n",
        "        # Mostra estatísticas finais\n",
        "        print(\"\\n=== Estatísticas Finais ===\")\n",
        "        print(f\"Total de registros processados: {history['processed_records']:,}\")\n",
        "        print(f\"Tempo total de processamento: {history['metrics']['processing_time']}\")\n",
        "\n",
        "        if history['metrics'].get('n_drifts'):\n",
        "            print(f\"Drifts detectados: {history['metrics']['n_drifts']}\")\n",
        "\n",
        "        # Mostra top artigos se disponível\n",
        "        if history['metrics'].get('top_articles'):\n",
        "            print(\"\\nTop Artigos:\")\n",
        "            top_articles = eval(history['metrics']['top_articles'])\n",
        "            for article, count in top_articles.items():\n",
        "                print(f\"Artigo {article}: {count:,} interações\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "1SRU_XJppiUoPKfWKdO2QK_M16SMpa80B",
      "authorship_tag": "ABX9TyNZQAwgq9ftk2wxXg0tIkbk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
